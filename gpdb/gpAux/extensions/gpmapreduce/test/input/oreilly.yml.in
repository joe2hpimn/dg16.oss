%YAML 1.1
---
VERSION: 1.0.0.1

EXECUTE:
  - RUN:
      SOURCE:  ProductionJobs
      MAP:     extractFields
      REDUCE:  Uniquify
      TARGET:  out_1
      
DEFINE:
  
  - OUTPUT:
      NAME:    out_1
      FILE:    @abs_srcdir@/output/oreilly.1
      DELIMITER: '|'
      
  - REDUCE:
      NAME:         Uniquify
      TRANSITION:   Uniquify_transition
      FINALIZE:     Uniquify_finalize

  - TRANSITION:
      NAME:         Uniquify_transition
      PARAMETERS:
        - state text
        - id integer
        - date_posted date
        - date_posted_timestamp integer
        - first_extract_date timestamp
        - last_extract_date timestamp
        - inter_source_hash text
        - gap integer
      LANGUAGE: python

      # The transition function just accumulates a list of delimited records
      FUNCTION:  |
        import pickle;
        
        # newrecord = (id, date_posted, date_posted_timestamp, refind_key,
        newrecord = (id, date_posted, date_posted_timestamp,
                     first_extract_date, last_extract_date, inter_source_hash)

        # Add the new record to the existing state, if any
        if state == None:
          oldlist = [gap, newrecord]
        else:
          oldlist = pickle.loads(state)
          oldlist.append(newrecord)

        # repickle and return
        return pickle.dumps(oldlist)

  - FINALIZE:
      NAME:         Uniquify_finalize
      RETURNS:
        - id                    integer
        - date_posted           date
        - date_posted_timestamp integer
        - first_extract_date    timestamp
        - last_extract_date     timestamp
        - inter_source_hash     text
        - refind_index          integer

      MODE: MULTI
      LANGUAGE: python
      FUNCTION: |
        import pickle
        import datetime

        def date_conv(date, fmt):
          return datetime.datetime.strptime(date, fmt)
        date_fmt = '%Y-%m-%d'
        
        # Get the accumulated data for this refind_key
        mydata = pickle.loads(state)

        # The first value in the list is the 'gap' as it was
        # passed to the transition function.
        gap = mydata.pop(0)
        if gap == None:
          gap = 60

        # derive the sequence
        mydata.sort(key=lambda(x): x[1])    # date_posted is 2nd element in each tuple
        
        # Determine which ones are 'new' values
        seq = 0
        prev = mydata[0]
        prev_date = date_conv(prev[1], date_fmt)
        for t in mydata[1:]:
          date_gap = date_conv(t[1], date_fmt) - prev_date;
          if (date_gap.days > gap):
            yield(prev[0], prev[1], prev[2], prev[3], prev[4], prev[5], seq)
            seq += 1
          prev = t
        yield(prev[0], prev[1], prev[2], prev[3], prev[4], prev[5], seq)
        

  - MAP:
      NAME:     extractFields
      PARAMETERS:
        - id integer
        - date_posted date
        - date_posted_timestamp integer
        - refind_key text
        - first_extract_date timestamp
        - last_extract_date timestamp
        - inter_source_hash text
      RETURNS:
        - id integer
        - date_posted date
        - date_posted_timestamp integer
        - refind_key text
        - first_extract_date timestamp
        - last_extract_date timestamp
        - inter_source_hash text
      MODE:     MULTI
      LANGUAGE: python
      FUNCTION: |
        if date_posted:
          yield [id, date_posted, date_posted_timestamp, refind_key,
                 first_extract_date, last_extract_date, inter_source_hash]
      
  - INPUT:
      NAME:      ProductionJobs
      FILE:        @hostname@:@abs_srcdir@/data/ProductionJobs.txt
      FORMAT:      CSV
      ERROR_LIMIT:  1000
      NULL:        '\\N'
      QUOTE:        '"'
      COLUMNS:
        - id integer
        - dummy1 text
        - dummy2 text
        - dummy3 text
        - dummy4 text
        - dummy5 text
        - dummy6 text 
        - dummy7 text
        - dummy8 text
        - dummy9 text
        - dummy10 text
        - dummy11 text
        - dummy12 text
        - dummy13 text
        - dummy14 text
        - dummy15 text
        - dummy16 text  
        - dummy17 text
        - dummy18 text
        - dummy19 text
        - dummy20 text
        - dummy21 text
        - dummy22 text
        - dummy23 text
        - dummy24 text
        - dummy25 text
        - dummy26 text  
        - dummy27 text
        - dummy28 text
        - dummy29 text
        - dummy30 text
        - dummy31 text
        - dummy32 text
        - dummy33 text
        - dummy34 text
        - dummy35 text
        - dummy36 text  
        - dummy37 text
        - dummy38 text
        - date_posted  date
        - dummy40 text
        - date_posted_timestamp integer
        - dummy42 text
        - dummy43 text
        - dummy44 text
        - dummy45 text
        - dummy46 text  
        - dummy47 text
        - dummy48 text
        - dummy49 text
        - dummy50 text
        - dummy51 text
        - dummy52 text
        - dummy53 text
        - dummy54 text
        - dummy55 text
        - dummy56 text  
        - dummy57 text
        - dummy58 text
        - dummy59 text
        - dummy60 text
        - dummy61 text
        - dummy62 text
        - dummy63 text
        - dummy64 text
        - dummy65 text
        - dummy66 text  
        - dummy67 text
        - dummy68 text
        - dummy69 text
        - dummy70 text
        - dummy71 text
        - dummy72 text
        - dummy73 text
        - refind_key text
        - first_extract_date timestamp
        - last_extract_date timestamp
        - dummy77 text
        - dummy78 text
        - dummy79 text
        - dummy80 text
        - dummy81 text
        - dummy82 text
        - dummy83 text
        - dummy84 text
        - dummy85 text
        - dummy86 text  
        - dummy87 text
        - dummy88 text
        - dummy89 text
        - dummy90 text
        - dummy91 text
        - inter_source_hash text
        - dummy93 text


