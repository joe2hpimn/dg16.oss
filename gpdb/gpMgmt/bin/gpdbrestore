#!/usr/bin/env python
#
# Copyright (c) Greenplum Inc 2008. All Rights Reserved. 
#
#
# THIS IMPORT MUST COME FIRST
#
# import mainUtils FIRST to get python version check
from gppylib.mainUtils import *

import os, sys
import fnmatch
from optparse import Option, OptionGroup, OptionParser, OptionValueError

try:
    from tempfile import mkstemp
    import gpmfr
    from gppylib.gpparseopts import OptParser, OptChecker

    from gppylib import pgconf
    from gppylib import userinput
    from gppylib.commands.base import Command
    from gppylib.commands.gp import Scp
    from gppylib.commands.unix import Ping
    from gppylib.db import dbconn
    from gppylib.gparray import GpArray
    from gppylib import gplog

    from gppylib.operations import Operation
    from gppylib.operations.backup_utils import check_backup_type, get_lines_from_file, generate_report_filename,\
                                           generate_dbdump_prefix, generate_master_dbdump_prefix,\
                                           generate_global_prefix, generate_createdb_prefix, \
                                           generate_createdb_filename, check_dir_writable
    from gppylib.operations.unix import CheckFile, CheckRemoteFile, ListFilesByPattern, ListRemoteFilesByPattern
    from gppylib.operations.utils import DEFAULT_NUM_WORKERS
    from gppylib.operations.restore import RestoreDatabase, RecoverRemoteDumps, ValidateTimestamp, GetDumpTables, \
                                           GetDbName, create_restore_plan, is_incremental_restore, DUMP_DIR, \
                                           is_begin_incremental_run, get_restore_tables_from_table_file,\
                                           validate_tablenames, get_restore_dir, restore_state_files_with_nbu, \
                                           restore_report_file_with_nbu, restore_cdatabase_file_with_nbu, \
                                           restore_config_files_with_nbu, restore_global_file_with_nbu, \
                                           restore_partition_list_file_with_nbu, restore_increments_file_with_nbu, \
                                           config_files_dumped, global_file_dumped, truncate_restore_tables
    import gppylib.operations.backup_utils as backup_utils
    import gppylib.operations.restore as restore

except ImportError, e:
    sys.exit('Cannot import modules.  Please check that you have sourced greenplum_path.sh.  Detail: ' + str(e))

# MPP-13617                                                                                                                                  
import re
RE1=re.compile('\\[([^]]+)\\]:(.+)')

WARN_MARK = '<<<<<'
logger = gplog.get_default_logger()

class GpdbRestore(Operation):
    def __init__(self, options, args):
        # only one of -t, -b, -R, and -s can be provided
        count = sum([1 for opt in ['db_timestamp', 'db_date_dir', 'db_host_path', 'search_for_dbname']
                       if options.__dict__[opt] is not None])
        if count == 0:
            raise ProgramArgumentValidationException("Must supply one of -t, -b, -R, or -s.")
        elif count > 1:
            raise ProgramArgumentValidationException("Only supply one of -t, -b, -R, or -s.")

        if options.list_tables and options.db_timestamp is None:
            raise ProgramArgumentValidationException("Need to supply -t <timestamp> for -L option")

        if options.list_backup and options.db_timestamp is None:
            raise ProgramArgumentValidationException("Need to supply -t <timestamp> for --list-backup option")

        if options.list_backup and options.drop_db:
            raise ProgramArgumentValidationException("Cannot specify --list-backup and -e together")

        if options.masterDataDirectory is None:
            options.masterDataDirectory = gp.get_masterdatadir()

        self.db_timestamp = options.db_timestamp
        self.list_tables = options.list_tables
        self.db_date_dir = options.db_date_dir
        self.db_host_path = options.db_host_path
        self.search_for_dbname = options.search_for_dbname.strip('"') if options.search_for_dbname is not None else options.search_for_dbname
        self.drop_db = options.drop_db
        self.restore_global = options.restore_global
        self.restore_tables = options.restore_tables
        self.batch_default = options.batch_default
        self.keep_dump_files = options.keep_dump_files
        self.no_analyze = options.no_analyze
        self.no_plan = options.no_plan
        self.no_ao_stats = options.no_ao_stats
        self.backup_dir = options.backup_dir
        self.table_file = options.table_file
        self.list_backup = options.list_backup
        self.redirected_restore_db = options.redirected_restore_db
        self.report_status_dir = options.report_status_dir
        self.truncate = options.truncate
        # NetBackup params
        self.netbackup_service_host = options.netbackup_service_host
        self.netbackup_block_size = options.netbackup_block_size
        if options.local_dump_prefix: 
            backup_utils.dump_prefix = options.local_dump_prefix + '_'

        if self.truncate and not (self.restore_tables or self.table_file):
            raise Exception('--truncate can be specified only with -T or --table-file option')

        if self.truncate and self.drop_db:
            raise Exception('Cannot specify --truncate and -e together')

        if self.table_file and self.restore_tables:
            raise Exception('Cannot specify -T and --table-file together')

        if self.list_backup and self.restore_tables:
            raise Exception('Cannot specify -T and --list-backup together')

        if self.restore_tables is not None:
            self.restore_tables = self.restore_tables.split(',')

        if self.table_file is not None:
            self.restore_tables = get_restore_tables_from_table_file(self.table_file) 

        if self.db_host_path is not None:
            # MPP-13617                                                                                                                      
            m = re.match(RE1, self.db_host_path)
            if m:
                self.db_host_path = m.groups()
            else:
                self.db_host_path = self.db_host_path.split(':')

        if self.db_host_path and self.netbackup_service_host:
            raise Exception('-R is not supported for restore with NetBackup.')

        if self.search_for_dbname and self.netbackup_service_host:
            raise Exception('-s is not supported with NetBackup.')

        if self.db_date_dir and self.netbackup_service_host:
            raise Exception('-b is not supported with NetBackup.')

        if self.list_tables and self.netbackup_service_host:
            raise Exception('-L is not supported with NetBackup.')

        # force non-interactive if list_backup
        if options.list_backup:
            self.interactive = False
        else:
            self.interactive = options.interactive

        self.master_datadir = options.masterDataDirectory
        self.master_port = self._get_master_port(self.master_datadir)

        self.gparray = None

        try:
            if self.report_status_dir:
                check_dir_writable(self.report_status_dir)
        except Exception as e:
            logger.warning('Directory %s should be writable' % self.report_status_dir)
            raise Exception(str(e))

        self.ddboost = options.ddboost
        if self.ddboost:
            if self.netbackup_service_host:
                raise Exception('--ddboost option is not supported with NetBackup')

            if self.db_host_path is not None:
                raise ExceptionNoStackTraceNeeded("-R cannot be used with DDBoost parameters.")
            elif self.backup_dir:
                raise ExceptionNoStackTraceNeeded('-u cannot be used with DDBoost parameters.')
            dd = gpmfr.DDSystem("local")
            self.dump_dir = dd.defaultBackupDir
            # Ugly hack to propagate backup directory different from "db_dumps".
            backup_utils.DUMP_DIR = self.dump_dir
        else:
            self.dump_dir = DUMP_DIR

    def execute(self):
        if self.ddboost:
            cmdline = 'gpddboost --sync --dir=%s' % self.master_datadir
            cmd = Command('DDBoost sync', cmdline)
            cmd.run(validateAfter = True)

        self.gparray = GpArray.initFromCatalog(dbconn.DbURL(port = self.master_port), utility=True)

        if self.netbackup_service_host:
            logger.info("Restoring metadata files with NetBackup")
            restore_state_files_with_nbu(self.master_datadir, self.backup_dir, self.db_timestamp, self.netbackup_service_host, self.netbackup_block_size)
            restore_report_file_with_nbu(self.master_datadir, self.backup_dir, self.db_timestamp, self.netbackup_service_host, self.netbackup_block_size)
            restore_cdatabase_file_with_nbu(self.master_datadir, self.backup_dir, self.db_timestamp, self.netbackup_service_host, self.netbackup_block_size)
            if config_files_dumped(self.master_datadir, self.backup_dir, self.db_timestamp, self.netbackup_service_host):
                restore_config_files_with_nbu(self.master_datadir, self.backup_dir, self.db_timestamp, self.master_port, self.netbackup_service_host, self.netbackup_block_size)
            if global_file_dumped(self.master_datadir, self.backup_dir, self.dump_dir, self.db_timestamp, self.netbackup_service_host):
                restore_global_file_with_nbu(self.master_datadir, self.backup_dir, self.dump_dir, self.db_timestamp, self.netbackup_service_host, self.netbackup_block_size)

        if self.list_tables and self.db_timestamp is not None:
            return self._list_dump_tables()

        info = self._gather_info()
        self._output_info(info)
        
        if self.interactive:
            if not userinput.ask_yesno(None, "\nContinue with Greenplum restore", 'N'):
                raise UserAbortedException()

        if self.db_host_path is not None:
            host, path = self.db_host_path
            RecoverRemoteDumps(host = host, path = path, 
                               restore_timestamp = info['restore_timestamp'],
                               compress = info['compress'],
                               restore_global = self.restore_global,
                               batch_default = self.batch_default,
                               master_datadir = self.master_datadir,
                               master_port = self.master_port).run()
    
            if is_incremental_restore(self.master_datadir, self.backup_dir, info['restore_timestamp']):
                raise Exception('-R is not supported for restore with incremental timestamp %s' % info['restore_timestamp'])

        if not is_incremental_restore(self.master_datadir, self.backup_dir, info['restore_timestamp']) and self.list_backup:
            raise Exception('--list-backup is not supported for restore with full timestamps')

        if self.restore_tables and self.truncate:
            if self.redirected_restore_db:
                truncate_restore_tables(self.restore_tables, self.master_port, self.redirected_restore_db)
            else:
                truncate_restore_tables(self.restore_tables, self.master_port, info['restore_db'])

        if is_begin_incremental_run(self.master_datadir, self.backup_dir, info['restore_timestamp'], self.no_plan):
            if self.netbackup_service_host:
                restore_partition_list_file_with_nbu(self.master_datadir, self.backup_dir, info['restore_timestamp'], self.netbackup_service_host, self.netbackup_block_size)
                restore_increments_file_with_nbu(self.master_datadir, self.backup_dir, info['restore_timestamp'], self.netbackup_service_host, self.netbackup_block_size)
            plan_file = create_restore_plan(self.master_datadir, self.backup_dir, info['restore_timestamp'], info['restore_db'], self.ddboost, self.dump_dir, self.netbackup_service_host, self.netbackup_block_size)

            if self.list_backup and self.db_timestamp is not None:
                return self._list_backup_timestamps(plan_file)

        RestoreDatabase(restore_timestamp = info['restore_timestamp'],
                        no_analyze = self.no_analyze,
                        drop_db = self.drop_db,
                        restore_global = self.restore_global,
                        master_datadir = self.master_datadir,
                        backup_dir = self.backup_dir,
                        master_port = self.master_port,
                        ddboost = self.ddboost,
                        dump_dir = self.dump_dir,
                        no_plan = self.no_plan,
                        restore_tables = self.restore_tables,
                        batch_default = self.batch_default,
                        no_ao_stats = self.no_ao_stats,
                        redirected_restore_db = self.redirected_restore_db,
                        report_status_dir = self.report_status_dir,
                        netbackup_service_host = self.netbackup_service_host,
                        netbackup_block_size = self.netbackup_block_size).run()

        if self.no_analyze:
            logger.warn('--------------------------------------------------------------------------------------------------')
            logger.warn('Analyze bypassed on request; database performance may be adversely impacted until analyze is done.')
            logger.warn('--------------------------------------------------------------------------------------------------')
    
        os._exit(0)

    def _output_info(self, info):
        logger.info("------------------------------------------")
        logger.info("Greenplum database restore parameters")
        logger.info("------------------------------------------")

        if is_incremental_restore(self.master_datadir, self.backup_dir, info['restore_timestamp']):
            if self.restore_tables is not None:
                logger.info("Restore type               = Incremental Table Restore")
            else: 
                logger.info("Restore type               = Incremental Restore")
        elif self.restore_tables is not None:
            logger.info("Restore type               = Table Restore")
        else: 
            logger.info("Restore type               = Full Database")

        if self.restore_tables is not None:
            logger.info("Database name              = %s" % info['restore_db'])
            logger.info("------------------------------------------")
            logger.info("Table restore list")
            logger.info("------------------------------------------")
            for restore_table in self.restore_tables:       
                logger.info("Table                      = %s" % restore_table)
            if info['table_counts']:
                logger.info("------------------------------------------")
                logger.warn("Following tables have non-zero row counts %s" % WARN_MARK)
                logger.info("------------------------------------------")
                for table, count in info['table_counts']:
                    logger.warn("Table:Row count            = %s:%s" % (table, count))
                logger.info("------------------------------------------")
        else:
            logger.info("Database to be restored    = %s" % info['restore_db'])
            if self.drop_db:
                logger.info("Drop and re-create db      = On")
            else:
                logger.info("Drop and re-create db      = Off")        

        if self.db_timestamp is not None and self.restore_tables is None:
            logger.info("Restore method             = Restore specific timestamp")
        if self.search_for_dbname is not None and self.restore_tables is None:
            logger.info("Restore method             = Search for latest")
        if self.redirected_restore_db is not None:
            logger.info("Redirect Restore database  = %s" % self.redirected_restore_db)
        if self.db_date_dir is not None and self.restore_tables is None:
            logger.info("Restore method             = Restore specific date")
        if self.restore_tables is not None:
            logger.info("Restore method             = Specific table restore")
        if self.db_host_path is not None:
            host, path = self.db_host_path
            logger.info("Restore method             = Remote host")
            logger.info("Recovery hostname          = %s" % host)
            logger.info("Remote recovery path       = %s" % path)
        logger.info("Restore timestamp          = %s" % info['restore_timestamp'])
        if info['compress']:
            logger.info("Restore compressed dump    = On")
        else:
            logger.info("Restore compressed dump    = Off")
        
        """
        TODO: These 3 lines are removed, b/c they pertain only to partial restore. 
        #logger.info("DBID List or restore       = %s" % dbid_list)
        #logger.info("Content list for restore   = %s" % content_list)
        #logger.info("Restore type               = %s" % restore_type)
        """

        if self.restore_global:
            logger.info("Restore global objects     = On")
        else:
            logger.info("Restore global objects     = Off")
        logger.info("Array fault tolerance      = %s" % info['fault_action'])
        logger.info("------------------------------------------")

    def _get_master_port(self, datadir):
        """ TODO: This function will be widely used. Move it elsewhere?
            Its necessity is a direct consequence of allowing the -d <master_data_directory> option. From this,
            we need to deduce the proper port so that the GpArrays can be generated properly. """
        logger.debug("Obtaining master's port from master data directory")
        pgconf_dict = pgconf.readfile(datadir + "/postgresql.conf")
        return pgconf_dict.int('port')

    def _list_dump_tables(self):
        dump_tables = GetDumpTables(master_datadir = self.master_datadir,
                                    backup_dir = self.backup_dir, 
                                    restore_timestamp = self.db_timestamp,
                                    dump_dir = self.dump_dir,
                                    ddboost = self.ddboost,
                                    netbackup_service_host = self.netbackup_service_host).run()

        logger.info("--------------------------------------------------------------------")
        logger.info("List of database tables for dump file with time stamp %s" % self.db_timestamp)
        logger.info("--------------------------------------------------------------------")
        for schema, table, owner in dump_tables:
            logger.info("Table %s.%s Owner %s" % (schema, table, owner))
        logger.info("--------------------------------------------------------------------")

    def _list_backup_timestamps(self, plan_file):
        plan_file_contents = get_lines_from_file(plan_file)
        logger.info("--------------------------------------------------------------------")
        logger.info("List of backup timestamps required for the restore time stamp %s" % self.db_timestamp)
        logger.info("--------------------------------------------------------------------")
        for line in plan_file_contents:
            logger.info("Backup Timestamp: %s" % (line.split(':')[0].strip()))
        logger.info("--------------------------------------------------------------------")

    def _gather_info(self):
        fault_action = self.gparray.getFaultStrategy()
        primaries = [seg for seg in self.gparray.getDbList() if seg.isSegmentPrimary(current_role=True)]
        total_primary_segments = len(primaries)
        fail_count = len([seg for seg in primaries if seg.isSegmentDown()])

        if fault_action == 'readonly' and fail_count != 0:
            logger.fatal("There are %d primary segment databases marked as invalid")
            logger.fatal("Array fault action is set to readonly, unable to initiate a restore")
            logger.info("Use gprecoverseg utility to recover failed segment instances")
            raise ExceptionNoStackTraceNeeded("Unable to continue")

        (restore_timestamp, restore_db, compress) = (None, None, None)
        if self.db_timestamp is not None:
            (restore_timestamp, restore_db, compress) = ValidateTimestamp(master_datadir = self.master_datadir,
                                                                          backup_dir = self.backup_dir, 
                                                                          candidate_timestamp = self.db_timestamp,
                                                                          dump_dir = self.dump_dir,
                                                                          netbackup_service_host = self.netbackup_service_host,
                                                                          ddboost = self.ddboost).run()
        elif self.db_date_dir is not None:
            (restore_timestamp, restore_db, compress) = self._validate_db_date_dir()
        elif self.db_host_path is not None:
            (restore_timestamp, restore_db, compress) = self._validate_db_host_path()
        elif self.search_for_dbname is not None:
            (restore_timestamp, restore_db, compress) = self._search_for_latest()

        if not self.drop_db:
            dburl = dbconn.DbURL(port = self.master_port)
            conn = dbconn.connect(dburl)
            count = dbconn.execSQLForSingleton(conn, "select count(*) from pg_database where datname='%s';" % restore_db)             
            # TODO: -e is needed even if the database doesn't exist yet?
            if count == 0:
                raise ExceptionNoStackTraceNeeded("Database %s does not exist and -e option not supplied" % restore_db)

        table_counts = []
        if self.restore_tables is not None :
            self.restore_tables = validate_tablenames(self.restore_tables)

        if not self.db_host_path and not self.ddboost:
            report_filename = generate_report_filename(self.master_datadir, self.backup_dir, restore_timestamp)
            if not os.path.isfile(report_filename):
                raise ExceptionNoStackTraceNeeded("Report file does not exist for the given restore timestamp %s: '%s'" % (restore_timestamp, report_filename))

        return {'fault_action': fault_action,
                'fail_count': fail_count,
                'restore_timestamp': restore_timestamp,
                'restore_db': restore_db,
                'compress': compress,
                'table_counts': table_counts}
                 

    def _validate_db_date_dir(self):
        root = os.path.join(get_restore_dir(self.master_datadir, self.backup_dir), self.dump_dir, self.db_date_dir)
        if not os.path.isdir(root):
            raise ExceptionNoStackTraceNeeded("Directory %s does not exist" % root)
        matching = ListFilesByPattern(root, "%s*" % generate_createdb_prefix()).run()
        if len(matching) == 0:
            raise ExceptionNoStackTraceNeeded("Could not locate Master database dump files under %s" % root)
        matching = sorted(matching, key=lambda x: int(x[len(generate_createdb_prefix()):].strip()))
        if len(matching) > 1:
            dates_and_times = []
            for match in matching:
                temp = match[len(generate_createdb_prefix()):]
                date, time = temp[0:8], temp[8:14]
                dates_and_times.append((date, time))
            restore_timestamp = self._select_multi_file(dates_and_times)
        else:
            match = matching[0]
            temp = match[len(generate_createdb_prefix()):]
            restore_timestamp = temp[0:14]
        restore_db = GetDbName(os.path.join(root, "%s%s" % (generate_createdb_prefix(), restore_timestamp))).run()
        compressed_file = "%s%s.gz" % (generate_master_dbdump_prefix(), restore_timestamp)
        compress = CheckFile(os.path.join(root, compressed_file)).run()
        return (restore_timestamp, restore_db, compress)

    def _validate_db_host_path(self):
        # The format of the -R option should be 'hostname:path_to_dumpset', hence the length should be 2 (hostname, path_to_dumpset)
        if len(self.db_host_path) != 2:
            raise ProgramArgumentValidationException("The arguments of the -R flag are incorrect. The correct form should be as follows:\nIPv4_address:path_to_dumpset\n-OR-\n[IPv6_address]:path_to_dumpset\n", False)
        
        host, path = self.db_host_path 
        logger.debug("The host is %s" % host)
        logger.debug("The path is %s" % path)    

        Ping.local('Pinging %s' % host, host)
        matching = ListRemoteFilesByPattern(path, "%s*" % generate_createdb_prefix(), host).run()
        if len(matching) == 0:
            raise ExceptionNoStackTraceNeeded("Could not locate Master database dump files at %s:%s" % (host,path))
        matching = sorted(matching, key=lambda x: int(x[len(generate_createdb_prefix()):].strip()))
        if len(matching) > 1:
            dates_and_times = []
            for match in matching:
                temp = match[len(generate_createdb_prefix()):]
                date, time = temp[0:8], temp[8:14]
                dates_and_times.append((date, time))
            restore_timestamp = self._select_multi_file(dates_and_times)
        else:
            match = matching[0]
            temp = match[len(generate_createdb_prefix()):]
            restore_timestamp = temp[0:14]

        # TODO: do a local GetDbName after Scp
        handle, filename = mkstemp() 
        srcFile = os.path.join(path, "%s%s" % (generate_createdb_prefix(), restore_timestamp))
        Scp('Copying cdatabase file', srcHost=host, srcFile=srcFile, dstFile=filename).run(validateAfter=True)
        restore_db = GetDbName(filename).run()
        os.remove(filename)

        compressed_file = "%s%s.gz" % (generate_master_dbdump_prefix(), restore_timestamp)
        compress = CheckRemoteFile(os.path.join(path, compressed_file), host).run() 
        return (restore_timestamp, restore_db, compress)
        
    def _search_for_latest(self):
        logger.info("Scanning Master host for latest dump file set for database %s" % self.search_for_dbname)
        root = os.path.join(get_restore_dir(self.master_datadir, self.backup_dir), self.dump_dir)
        candidates, timestamps = [], []
        for path, dirs, files in os.walk(root):
            matching = fnmatch.filter(files, "%s*" % generate_createdb_prefix())
            candidates.extend([(path, filename) for filename in matching])
        if len(candidates) == 0:
            raise ExceptionNoStackTraceNeeded("No %s* files located" % generate_createdb_prefix())
        for path, filename in candidates:
            db_name = GetDbName(os.path.join(path, filename)).run()
            if self.search_for_dbname == db_name:
                logger.info('Located dump file %s for database %s, adding to list' % (filename, self.search_for_dbname))
                timestamps.append(int(filename[len(generate_createdb_prefix()):]))
            else:
                logger.info("Dump file has incorrect database name of %s, skipping..." % db_name)
        if len(timestamps) == 0:
            raise ExceptionNoStackTraceNeeded("No %s* files located with database %s" % (generate_createdb_prefix(), self.search_for_dbname))
        restore_timestamp = str(max(timestamps)) 
        logger.info("Identified latest dump timestamp for %s as %s" %  (self.search_for_dbname, restore_timestamp))
        restore_db = self.search_for_dbname
        compressed_file = os.path.join(get_restore_dir(self.master_datadir, self.backup_dir), self.dump_dir, restore_timestamp[0:8], "%s%s.gz" % (generate_master_dbdump_prefix(), restore_timestamp))
        compress = CheckFile(compressed_file).run()
        return (restore_timestamp, restore_db, compress)
        
    def _select_multi_file(self, dates_and_times):
        spc = "        "
        def info(msg):
            print "%s%s" % (spc, msg)
        info("Select required dump file timestamp to restore")
        info(" #           Date    Time")
        info("--------------------------")
        for i in range(0, len(dates_and_times)):
            date, time = dates_and_times[i]
            info("[%d] ...... %s %s" % (i, date, time))
        info("Enter timestamp number to restore >")
        choice = raw_input()
        if choice == '':
            raise ExceptionNoStackTraceNeeded("Invalid or null input")
        if not choice.isdigit():
            raise ExceptionNoStackTraceNeeded("Invalid or null input")
        choice = int(choice)
        if choice < 0 or choice >= len(dates_and_times):
            raise ExceptionNoStackTraceNeeded("Invalid or null input")
        date, time = dates_and_times[choice]
        return "%s%s" % (date, time)

def create_parser():
    parser = OptParser(option_class=OptChecker, 
                       version='%prog version $Revision$',
                       description="Restores a Greenplum database or tables from a dumpset generated by gpdbdump")

    parser.setHelp([])                      # TODO: make gpparseopts do this
    addStandardLoggingAndHelpOptions(parser, includeNonInteractiveOption=True)

    addTo = OptionGroup(parser, 'Connection opts')
    parser.add_option_group(addTo)
    addMasterDirectoryOptionForSingleClusterProgram(addTo)

    addTo = OptionGroup(parser, 'Restore options: ')
    addTo.add_option('-t', dest='db_timestamp', metavar='<timestamp>', 
                     help="Timestamp key of backup file set that should be used for restore. Mandatory if -b, -R, -s are not supplied. Expects dumpset to reside on Greenplum array.")
    addTo.add_option('-L', action='store_true', dest='list_tables', default=False,
                     help="List table names in dump file, can only be used with -t <timestamp> option. Will display all tables in dump file, and then exit.")
    addTo.add_option('-b', dest='db_date_dir', metavar='<YYYYMMDD>',
                     help="%s/<YYYYMMDD> directory where dump files are located on the Greenplum array. Mandatory if -t, -s, -R options are not supplied." % DUMP_DIR)
    addTo.add_option('-R', dest='db_host_path', metavar='<hostname:dir_path>', 
                     help="Hostname and full directory path where backup set is located. Utility will recover files to master and segment hosts, and then start restore process.")
    addTo.add_option('-s', dest='search_for_dbname', metavar='<database name>', 
                     help="Search for latest backup set for the named database. Mandatory if -t, -b -R options are not supplied.")
    addTo.add_option('-e', action='store_true', dest='drop_db', default=False, 
                     help="Drop (erase) target database before recovery commences")
    addTo.add_option('-B', dest='batch_default', type='int', default=DEFAULT_NUM_WORKERS, metavar="<number>",
                     help="Dispatches work to segment hosts in batches of specified size [default: %s]" % DEFAULT_NUM_WORKERS)
    addTo.add_option('-G', action='store_true', dest='restore_global', default=False,
                     help="Restore global objects dump file if found in target dumpset. The global objects file is secured via the gpcrondump -G options, and has a filename of the form %s<timestamp>" % generate_global_prefix())
    addTo.add_option('-T', dest='restore_tables', metavar='<schema.tablename>', 
                     help="Restore a single table from a backup set. For multiple tables, supply a comma separated list of schema.tablenames. Note, table schema must exist in target database.")
    addTo.add_option('-K', action='store_true', dest='keep_dump_files', default=False,
                     help="Retain temporary generated table dump files.")
    addTo.add_option('--noanalyze', action='store_true', dest='no_analyze', default=False, 
                     help="Suppress the ANALYZE run following a successful restore. The user is responsible for running ANALYZE on any restored tables; failure to run ANALYZE following a restore may result in poor databse performance.")
    addTo.add_option('--noplan', action='store_true', dest='no_plan', default=False)
    addTo.add_option('--noaostats', action='store_true', dest='no_ao_stats', default=False)
    addTo.add_option('-u', dest='backup_dir', metavar='<backup_dir>', 
                     help="Full directory path where backup set is located.")
    addTo.add_option('--table-file', dest='table_file', metavar='<filename>',
                      help='Filename containing the names of tables to be restored')
    addTo.add_option('--list-backup', action='store_true', dest='list_backup', default=False,
                     help="List backup timestamps that are required to restore from the input timestamp. The input timestamp should be of an incremental backup")
    addTo.add_option('--prefix', dest='local_dump_prefix', default='', metavar='<filename prefix>',
                     help="Prefix of the dump files to be restored")
    addTo.add_option('--redirect', dest='redirected_restore_db', metavar='<database name>',
                     help="Database name to which the data needs to be restored for the given timestamp")
    addTo.add_option('--report-status-dir', dest='report_status_dir', metavar='<report_status_dir>',
                     help="Writable directory for report and status file creation")
    addTo.add_option('--truncate', action='store_true', dest='truncate', default=False,
                     help="Truncate's the restore tables specified using -T and --table-file option")
    addTo.add_option('--netbackup-service-host', dest='netbackup_service_host', metavar="<server name>",
                     help="NetBackup service hostname")
    addTo.add_option('--netbackup-block-size', dest='netbackup_block_size', metavar="<block size>",
                     help="NetBackup data transfer block size")
    parser.add_option_group(addTo)

    ddOpt = OptionGroup(parser, "DDBoost")
    ddOpt.add_option('--ddboost', dest='ddboost', help="Dump to DDBoost using ~/.ddconfig", action="store_true", default=False)
    parser.add_option_group(ddOpt)

    return parser

if __name__ == '__main__':
    simple_main(create_parser, GpdbRestore)
