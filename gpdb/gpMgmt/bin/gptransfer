#!/usr/bin/env python
# -*- coding: utf-8 -*-

# Builtin function use - pylint: disable=W0141


# Copyright (c) 2013 Pivotal Software, Inc. All Rights Reserved
#
# This software contains the intellectual property of Pivotal Software, Inc.
# or is licensed to Pivotal Software, Inc. from third parties. Use of this
# software and the intellectual property contained therein is expressly
# limited to the terms and conditions of the License Agreement under which
# it is provided by or on behalf of Pivotal Software, Inc.

"""
gptransfer utility is for tranfering one GPDB system to another. Currently
only table data or the entire system can be transfered.  In the future this
transfer will include dependent roles, UDFs, UDTs, views, resource queues,
and possibly a subset of system configuration of specified tables.
"""

import sys
import os
import re
import time
import signal
import inspect
import hashlib
import datetime
from threading import Thread, Event, Lock
import thread
from optparse import OptionGroup, SUPPRESS_HELP

try:
    from gppylib.mainUtils import simple_main, addStandardLoggingAndHelpOptions, \
        ProgramArgumentValidationException
    from gppylib.gplog import get_default_logger
    from gppylib.gpparseopts import OptParser, OptChecker
    from gppylib.commands.base import Command, LOCAL, REMOTE, WorkerPool, \
        ExecutionError, CommandResult
    from gppylib.commands.unix import MakeDirectory, RemoveDirectory, \
        RemoveFiles, FileDirExists, Scp, LINUX, curr_platform
    from gppylib.commands.gp import get_gphome
    from gppylib.db.dbconn import connect, DbURL, execSQL, \
        execSQLForSingletonRow
    from gppylib.db.catalog import getUserDatabaseList, doesSchemaExist, dropSchemaIfExist
    from gppylib.gparray import GpArray
    from gppylib.userinput import ask_yesno
    from pygresql.pg import DB
except ImportError, import_exception:
    sys.exit('Cannot import modules.  Please check that you have sourced'
             ' greenplum_path.sh.  Detail: %s' % str(import_exception))

# --------------------------------------------------------------------------
# pydoc variables
# --------------------------------------------------------------------------
__author__ = 'Chris Pedrotti <cpedrotti@gopivotal.com>'
__date__ = '1 February 2014'
__version__ = '0.0.1'


# --------------------------------------------------------------------------
# Description, help and usage constants
# --------------------------------------------------------------------------

__description__ = \
    """Transfers data from one Greenplum Database system to another."""

__help__ = []

# --------------------------------------------------------------------------
# Global Constants
# --------------------------------------------------------------------------

EXECNAME = os.path.split(__file__)[-1]
GPTRANSFER_PID_FILE = 'gptransfer.pid'

DEFAULT_BASE_WORKDIR=os.path.expanduser('~/')
GPTRANSFER_TMP_DIR = 'gptransfer_%d' % os.getpid()

now = datetime.datetime.now()
tableCountLock = Lock()
remaining_tables = 0
GPTRANSFER_FAILED_TABLES_FILE = 'failed_transfer_tables_%s.txt' % now.strftime(
    '%Y%m%d_%H%M%S')

DEFAULT_BATCH_SIZE = 2
MAX_BATCH_SIZE = 10
DEFAULT_SUB_BATCH_SIZE = 25
MAX_SUB_BATCH_SIZE = 50

SCHEMA_DELIMITER='.'

DEFAULT_PORT = 5432
DEFAULT_USER = 'gpadmin'

DEFAULT_WAIT_TIME=3

DEFAULT_MAX_GPFDIST_INSTANCES = 1
DEFAULT_GPFDIST_MAX_LINE_LENGTH = 1024 * 1024 * 10  # (10MB)
MIN_GPFDIST_MAX_LINE_LENGTH = 1024 * 32  # (32KB)
MAX_GPFDIST_MAX_LINE_LENGTH = 1024 * 1024 * 256  # (256MB)
DEFAULT_GPFDIST_TIMEOUT = 300
DEFAULT_GPFDIST_BASE_PORT = 8000
DEFAULT_GPFDIST_LAST_PORT = -1
MIN_GPFDIST_TIMEOUT = 2
MAX_GPFDIST_TIMEOUT = 600
GPFDIST_PORT_REGEX = r"Serving HTTP on port (\d+)"


# --------------------------------------------------------------------------
# Logging
# --------------------------------------------------------------------------

logger = get_default_logger()


# --------------------------------------------------------------------------
# Global cancel flag and SIG method
# --------------------------------------------------------------------------

canceled = False
running_gpfdists = False

def wait_until_set_or_cancel(evt):
    while True:
        evt.wait(1)
        if evt.isSet() or canceled:
            return

def cancel_handler(_sig, _frame):  # pylint: disable=W0613
    """
    Sets canceled flag which starts the cancel process
    """
    global canceled
    logger.info('Canceling.  This may take some time...')
    canceled = True


def dump_threads(_sig, _frame):  # pylint: disable=W0613
    """
    Debug method that dumps threads
    """
    import traceback
    code = []
    for thread_id, stack in sys._current_frames().items():
        code.append("\n# ThreadID: %s" % thread_id)
        for filename, lineno, name, line in traceback.extract_stack(stack):
            code.append('File: "%s", line %d, in %s' %
                        (filename, lineno, name))
            if line:
                code.append("  %s" % (line.strip()))
    print "\n".join(code)


def dump_resource_usage(_sig, _frame):  # pylint: disable=W0613
    """
    Debug method that dumps resource usage
    """
    from resource import getrusage, RUSAGE_SELF

    keys = ['utime', 'stime', 'maxrss', 'ixrss', 'idrss',
            'isrss', 'minflt', 'majflt', 'nswap', 'inblock',
            'oublock', 'msgsnd', 'msgrcv', 'nsignals', 'nvcsw', 'nivcsw']
    usage = dict(zip(keys, list(getrusage(RUSAGE_SELF))))

    for key, val in usage.iteritems():
        logger.info("%s: %s", key, val)


# Set cancel on SIGINT
signal.signal(signal.SIGINT, cancel_handler)
# Set dump_threads on SIGUSR1
signal.signal(signal.SIGUSR1, dump_threads)
# Set dump_resource_usage on SIGUSR2
signal.signal(signal.SIGUSR2, dump_resource_usage)


# --------------------------------------------------------------------------
# Command line option parser
# --------------------------------------------------------------------------

def create_parser():
    """
    Creates the command line option parser for use by
    gppylib.mainUtils.simple_main
    """
    parser = OptParser(option_class=OptChecker,
                       version='%prog version $Revision: #1 $',
                       description=__description__)

    addStandardLoggingAndHelpOptions(parser,
                                     includeNonInteractiveOption=True)

    # General Options ####
    general_option_group = OptionGroup(parser, 'General Options')
    general_option_group.add_option(
        '--dry-run',
        dest='dry_run',
        default=False,
        action='store_true',
        help='Show what gptransfer will do without actually doing it'
    )
    general_option_group.add_option(
        '--batch-size',
        type='int',
        dest='batch_size',
        default=DEFAULT_BATCH_SIZE,
        action='store',
        metavar='<batch_size>',
        help='Table transfer concurrency (number of tables to transfer at one'
             ' time) [default: %d, maximum: %d]'
             % (DEFAULT_BATCH_SIZE, MAX_BATCH_SIZE)
    )
    general_option_group.add_option(
        '--sub-batch-size',
        type='int',
        dest='sub_batch_size',
        default=DEFAULT_SUB_BATCH_SIZE,
        action='store',
        metavar='<sub_batch_size>',
        help='Transfer operations concurrency [default: %d, maximum: %d]'
             % (DEFAULT_SUB_BATCH_SIZE, MAX_SUB_BATCH_SIZE)
    )
    general_option_group.add_option(
        '--enable-test',
        dest='enable_test',
        action='store_true',
        default=False,
        help=SUPPRESS_HELP
    )
    general_option_group.add_option(
        '--max-line-length',
        type='int',
        dest='max_line_length',
        default=DEFAULT_GPFDIST_MAX_LINE_LENGTH,
        action='store',
        metavar='<length>',
        help='Maximum line length for gpfdist [default: %d, minimum: %d,'
             ' maximum: %d]' % (DEFAULT_GPFDIST_MAX_LINE_LENGTH,
                                MIN_GPFDIST_MAX_LINE_LENGTH,
                                MAX_GPFDIST_MAX_LINE_LENGTH)
    )
    general_option_group.add_option(
        '--timeout',
        type='int',
        dest='timeout',
        default=DEFAULT_GPFDIST_TIMEOUT,
        action='store',
        metavar='<timeout>',
        help='gpfdist timeout in seconds [default: %d, minimum: %d, maximum: %d]'
            % (DEFAULT_GPFDIST_TIMEOUT, MIN_GPFDIST_TIMEOUT, MAX_GPFDIST_TIMEOUT)
    )
    general_option_group.add_option(
        '--delimiter',
        type='string',
        default= ',',
        action='store',
        metavar='<delim>',
        help='Delimiter to use for external tables'
    )
    general_option_group.add_option(
        '--wait-time',
        type='int',
        dest='wait_time',
        default=DEFAULT_WAIT_TIME,
        action='store',
        metavar='<wait time>',
        help=SUPPRESS_HELP
    )
    general_option_group.add_option(
        '--force-standard-mode',
        dest='force_standard_mode',
        default=False,
        action='store_true',
        help=SUPPRESS_HELP
    )
    general_option_group.add_option(
        '--format',
        dest='format',
        default='CSV',
        action='store',
        choices=['CSV', 'csv', 'TEXT','text'],
        help='Transfer data in CSV(default) or TEXT format'
    )
    general_option_group.add_option(
        '--quote',
        dest='quote',
        default='\001',
        action='store',
        help='Specifies the quotation character for CSV mode. The default is "\001".'
    )

    parser.add_option_group(general_option_group)

    # Source Options ####
    source_option_group = OptionGroup(parser, 'Source Options')

    # source host
    source_option_group.add_option(
        '--source-host',
        type='string',
        dest='source_host',
        default='127.0.0.1',
        action='store',
        metavar='<host>',
        help='Source Greenplum Database hostname or IP address'
    )
    # source port
    source_option_group.add_option(
        '--source-port',
        type='int',
        dest='source_port',
        default=DEFAULT_PORT,
        action='store',
        metavar='<port>',
        help='Source Greenplum Database port number'
    )
    # source addresses file
    source_option_group.add_option(
        '--source-map-file',
        type='string',
        dest='source_map_file',
        default=None,
        action='store',
        metavar='<file>',
        help='File containing all source hostname to address mappings'
    )
    # source user
    source_option_group.add_option(
        '--source-user',
        type='string',
        dest='source_user',
        default=DEFAULT_USER,
        action='store',
        metavar='<user>',
        help='User to connect to source Greenplum Database as'
    )
    # base port for gpfdist instances
    source_option_group.add_option(
        '--base-port',
        type='int',
        dest='base_port',
        default=DEFAULT_GPFDIST_BASE_PORT,
        action='store',
        metavar='<base_port>',
        help='Base port for gpfdist on source systems',
    )
    source_option_group.add_option(
        '--last-port',
        type='int',
        dest='last_port',
        default=DEFAULT_GPFDIST_LAST_PORT,
        action='store',
        metavar='<last_port>',
        help=SUPPRESS_HELP
    )
    # full
    source_option_group.add_option(
        '--full',
        action='store_true',
        default=False,
        help='Full transfer of the entire GPDB source system to destination'
    )
    # tables
    source_option_group.add_option(
        '-t',
        type='string',
        dest='tables',
        default=[],
        action='append',
        metavar='<db.schema.table>',
        help='Table to transfer.  Can be specified multiple times to include '
             'multiple tables'
    )

    # tables
    source_option_group.add_option(
        '-T',
        type='string',
        dest='exclude_tables',
        default=[],
        action='append',
        metavar='<db.schema.table>',
        help='Table to exclude from transfer.  Can be specified multiple times to exclude '
             'multiple tables'
    )


    # database
    source_option_group.add_option(
        '-d',
        type='string',
        dest='databases',
        default=[],
        action='append',
        metavar='<database>',
        help='Database to transfer.  Can be specified multiple times to '
             'include multiple databases'
    )
    # file
    source_option_group.add_option(
        '-f',
        type='string',
        dest='input_file',
        default=None,
        action='store',
        metavar='<file>',
        help='File containing list of database.schema.table entries to transfer'
    )
    # file
    source_option_group.add_option(
        '-F',
        type='string',
        dest='exclude_input_file',
        default=None,
        action='store',
        metavar='<file>',
        help='File containing list of database.schema.table entries to exclude from transfer'
    )
    # exclusive lock
    source_option_group.add_option(
        '-x',
        dest='exclusive_lock',
        default=False,
        action='store_true',
        help='Acquire exclusive lock on source table during the transfer '
             'to prevent insert/updates'
    )
    # work base directory
    source_option_group.add_option(
        '--work-base-dir',
        type='string',
        dest='work_base_dir',
        default=DEFAULT_BASE_WORKDIR,
        action='store',
        metavar='<directory>',
        help='Base directory to create gptransfer work directory'
    )

    # max gpfdist instances per host
    source_option_group.add_option(
        '--max-gpfdist-instances',
        type='int',
        dest='max_gpfdist_instances',
        default=DEFAULT_MAX_GPFDIST_INSTANCES,
        action='store',
        metavar='<instance count>',
        help=SUPPRESS_HELP  # 'Maximum gpfdist instances per host'
    )

    parser.add_option_group(source_option_group)

    # Dest Options ####
    dest_option_group = OptionGroup(parser, 'Destination Options')

    # destination host
    dest_option_group.add_option(
        '--dest-host',
        type='string',
        dest='dest_host',
        default='127.0.0.1',
        action='store',
        metavar='<host>',
        help='Destination Greenplum Database hostname or IP address'
    )
    # destination port
    dest_option_group.add_option(
        '--dest-port',
        type='int',
        dest='dest_port',
        default=DEFAULT_PORT,
        action='store',
        metavar='<port>',
        help='Destination Greenplum Database port number'
    )
    # destination user
    dest_option_group.add_option(
        '--dest-user',
        type='string',
        dest='dest_user',
        default=DEFAULT_USER,
        action='store',
        metavar='<user>',
        help='User to connect to destination Greenplum Database as'
    )
    # destination database
    dest_option_group.add_option(
        '--dest-database',
        type='string',
        dest='dest_database',
        default=None,
        action='store',
        metavar='<database>',
        help='Database to transfer all source tables into'
    )
    # skip tables that exist
    dest_option_group.add_option(
        '--skip-existing',
        dest='skip_existing',
        default=False,
        action='store_true',
        help='Skip tables that exist in destination Greenplum Database '
             'instead of erroring out'
    )
    # truncate tables that exist
    dest_option_group.add_option(
        '--truncate',
        default=False,
        action='store_true',
        help='Truncate destination table if it exists prior to transferring data'
    )

    # drop tables that exist
    dest_option_group.add_option(
        '--drop',
        default=False,
        action='store_true',
        help='Drop destination table if it exists prior to transferring data'
    )

    # analyze destination table after transfer
    dest_option_group.add_option(
        '--analyze',
        default=False,
        action='store_true',
        help='Analyze table after transfer'
    )

    dest_option_group.add_option(
        '--schema-only',
        dest='schema_only',
        default=False,
        action='store_true',
        help='Create schema only.  Do not transfer data'
    )

    parser.add_option_group(dest_option_group)

    # Validation Options ####
    validation_option_group = OptionGroup(parser, 'Validation Options')

    validation_option_group.add_option(
        '--validate',
        type='string',
        dest='validator',
        default=None,
        action='store',
        metavar='<validator>',
        help='Method to validate source and destination tables. '
             '[Available validators: %s]' %
             ', '.join(validator_factory.get_available_validators())
    )

    parser.add_option_group(validation_option_group)

    parser.setHelp(__help__)

    return parser

def wait_for_pool(pool, max_queued, wait_until_empty):
    """
    Waits for pool to empty below the number of workers.  This helps prevent
    high memory usage by queueing up lots of commands.

    pool: pool to wait on
    max_queued: maximum number of commands.  Once number queued drops below this
                it returns completed commands
    wait_until_empty: wait until the work queue is empty

    returns: successful and failed commands
    """
    successful_commands = list()
    failed_commands = list()
    # TODO: we should not be doing this but by using the max queue items.
    #       Unfortunately this is an arg to Queue() which isn't
    #       exposed through our worker pool class yet.
    while pool.work_queue.qsize() >= max_queued or wait_until_empty:
        if wait_until_empty:
            pool.join()

        has_completed_cmds = False
        for cmd in pool.getCompletedItems():
            has_completed_cmds = True
            res = cmd.get_results()
            if not res.wasSuccessful():
                failed_commands.append(cmd)
            else:
                successful_commands.append(cmd)
        if (has_completed_cmds and not (wait_until_empty and pool.work_queue.qsize() == 0)) or \
            ((canceled or wait_until_empty) and pool.work_queue.qsize() == 0):
            break
        time.sleep(1)

    return (successful_commands, failed_commands)

# --------------------------------------------------------------------------

def split_fqn(fqn):
    """
    Splits a fully qualified database table name (<db>.<schema>.<table>)
    into its three components.
    """

    index = 0
    partitions = ['','','']
    regexp_parsing = False

    for c in fqn:
        if index > 2:
            break
        if c != '/' and c != SCHEMA_DELIMITER:
            partitions[index] = partitions[index] + c
        elif c == '/':
            partitions[index] = partitions[index] + c
            if not regexp_parsing:
                regexp_parsing = True
            else:
                regexp_parsing = False
        else:
            if not regexp_parsing:
                index += 1
            else:
                partitions[index] = partitions[index] + c

    if index != 2:
        raise Exception('Invalid fully qualified table name %s' % fqn)

    (database, schema, table) = partitions

    return (database, schema, table)

def _process_regexp(regexp=None):
    """
    Add a dot before patterns which require preceding expression
    Add ^ and $ to match from begin to the end of line.
    """

    if regexp is None:
        return regexp

    regexp = validate_regexp(regexp)

    if not regexp.startswith('^'):
        regexp = '^' + regexp
    if not regexp.endswith('$'):
        regexp = regexp + '$'

    return regexp


def validate_regexp(regexp=None):
    """
    validating the user input and parse out the regular experssion
    """

    if regexp.count('/') % 2 != 0:
        raise Exception('Regular expression should be surround with /')

    return regexp.replace('/', '')


def get_user_databases(host, port, user):
    """
    get all user databases in the gpdb system
    """
    all_dbs = list()

    url = DbURL(host, port, 'template1', user)
    conn = connect(url)
    all_dbs = [db[0] for db in getUserDatabaseList(conn)]
    conn.close()

    return all_dbs


def get_databases_by_regexp(host, port, user, databases, warning=False):
    """
    Retrieves a list of all databases based on matching the regular expressions.
    """
    dbs = list()
    all_dbs = get_user_databases(host, port, user)

    for database in databases:
        db_matched = False
        for db in all_dbs:
            try:
                if re.match(r'%s' % database, db, re.L|re.U) and db not in dbs:
                    dbs.append(db)
                    db_matched = True
            except Exception, e:
                raise Exception("Error matching regular expression %s with %s\n%s" %(database, db, e))
        if not db_matched and warning:
            logger.warning('Find no user databases matching "%s" in source system' % database[1 : len(database) - 1])

    return dbs

def get_user_tables(host, port, user, databases=None, full=False):
    """
    Retrieves all tables from a list of databases on a GPDB system.

    host: hostname of GPDB system
    port: port of the GPDB system
    user: username to use to connect to GPDB system
    full: True if to retrieve tables of all databases
    """

    # We need to use this SQL query because we only want the top level
    # partition.  If we use gpcatalog.get_usertable_list we will dupe data.
    all_tables_sql = """
SELECT
    n.nspname, c.relname, c.relstorage
FROM
    pg_class c JOIN pg_namespace n ON (c.relnamespace=n.oid)
    JOIN pg_catalog.gp_distribution_policy p on (c.oid = p.localoid)
WHERE
    c.oid NOT IN ( SELECT parchildrelid as oid FROM pg_partition_rule )
AND n.nspname NOT IN ('gpexpand', 'pg_bitmapindex', 'information_schema', 'gp_toolkit');"""

    all_tables = set()

    if full:
        databases = get_user_databases(host, port, user)
    elif databases is None or len(databases) == 0:
        return all_tables

    # get all the tables in the source system
    for database in databases:
        url = DbURL(host, port, database, user)
        conn = connect(url)
        cur = execSQL(conn, all_tables_sql)
        for row in cur:
            schema = row[0]
            table = row[1]
            external = (row[2] == 'x')
            all_tables.add(GpTransferTable(database, schema, table, external))
        cur.close()
        conn.close()
    return all_tables


def is_table_external(host, port, user, database, schema, table):
    """
    Checks if the table is an external table
    """
    is_external_sql = """
SELECT
    c.relstorage = 'x'
FROM
    pg_class c JOIN pg_namespace n ON (c.relnamespace=n.oid)
WHERE c.relname = '%s' AND n.nspname = '%s'"""
    conn = connect(DbURL(host, port, database, user))
    res = execSQLForSingletonRow(conn, is_external_sql % (table, schema))
    conn.close()
    return res[0]


def schema_exists_on_system(host, port, user, schema, databases=None):
    """
    host: hostname or ip address of Greenplum DB
    port: port of Greenplum DB
    user: user to connect as
    schema: schema name to search for
    databases: list of databases to search for schema

    Returns True if schema exists in the user database
    """

    if databases is None:
        url = DbURL(host, port, 'template1', user)
        conn = connect(url)
        databases = [db[0] for db in getUserDatabaseList(conn)]
        conn.close()

    for database in databases:
        url = DbURL(host, port, database, user)
        conn = connect(url)
        schema_exists = doesSchemaExist(conn, schema)
        conn.close()
        if schema_exists:
            return True

def drop_existing_schema_on_system(host, port, user, schema, databases=None):
    """  
    host: hostname or ip address of Greenplum DB
    port: port of Greenplum DB
    user: user to connect as
    schema: schema name to search for
    databases: list of databases to search for schema
    """

    if databases is None:
        url = DbURL(host, port, 'template1', user)
        conn = connect(url)
        databases = [db[0] for db in getUserDatabaseList(conn)]
        conn.close()

    for database in databases:
        url = DbURL(host, port, database, user)
        conn = connect(url)
        schema_exists = dropSchemaIfExist(conn, schema)
        conn.close()
        if schema_exists:
            return True

# --------------------------------------------------------------------------
# Validation classes
# --------------------------------------------------------------------------

class TableValidator(object):

    """
    Base class for table validators.
    """

    def __init__(self, work_dir, src_conn, dest_conn, src_sql, dest_sql):
        """
        src_conn: Source database connection
        dest_conn: Destination database connection
        src_sql: SQL to execute on source database
        dest_sql: SQL to execute on the destination database
        """

        self._src_conn = src_conn
        self._dest_conn = dest_conn
        self._src_sql = src_sql
        self._dest_sql = dest_sql
        self._src_res = None
        self._dest_res = None
        self._src_failed = False
        self._dest_failed = False
        self._work_dir = work_dir

    @staticmethod
    def setup(conn):
        """
        Child classes that need to perform some setup, such as create a UDF,
        should override this method.

        conn: Database connection to use for any setup SQL statements
        """
        pass

    @staticmethod
    def cleanup(conn):
        """
        Child classes that need to perform some cleanup, such as drop a UDF,
        should override this method.

        conn: Database connection to use for any cleanup SQL statements
        """
        pass

    def validate(self):
        """
        Starts threads that perform the validation SQL statements on the source
        and destination systems and waits for those threads to complete.  Also
        handles the canceling of validation.  In most cases child classes will
        not need to override this method.
        """

        source_thread = Thread(target=self._src_proc)
        source_thread.start()
        dest_thread = Thread(target=self._dest_proc)
        dest_thread.start()

        while True:
            if not source_thread.isAlive():
                if self._src_failed == True:
                    DB(self._dest_conn).cancel()
                    break

            if not dest_thread.isAlive():
                if self._dest_failed == True:
                    DB(self._src_conn).cancel()
                    break

            if not dest_thread.isAlive() and not source_thread.isAlive():
                break

            if canceled == True:
                try:
                    DB(self._src_conn).cancel()
                except:
                    pass
                try:
                    DB(self._dest_conn).cancel()
                except:
                    pass
                break

            time.sleep(1)

        source_thread.join()
        dest_thread.join()

        return self._compare()

    @staticmethod
    def get_name():
        """
        Gets the name of the validator which will be used as a valid arg to the
        --validator option.  The name must be unique amongst all validators.
        """
        return None

    def _src_proc(self):
        """
        Thread proc that executes the SQL statement on the source side.  To
        use this method the SQL statement must only return a single row.  If
        your query requires more complexity you should override this.
        """

        try:
            self._src_res = execSQLForSingletonRow(self._src_conn,
                                                   self._src_sql)
        except:
            self._src_failed = True

    def _dest_proc(self):
        """
        Thread proc that executes the SQL statement on the destination side.  To
        use this method the SQL statement must only return a single row.  If
        your query requires more complexity you should override this.
        """

        try:
            self._dest_res = execSQLForSingletonRow(self._dest_conn,
                                                    self._dest_sql)
        except:
            self._dest_failed = True

    def _compare(self):
        """
        Simple compare function that checks the source and destination rows
        returned are the same.
        """

        if not self._src_res or not self._dest_res:
            return False
        return self._src_res == self._dest_res


# --------------------------------------------------------------------------

class CountTableValidator(TableValidator):

    """
    Simple COUNT(*) "validation".
    """

    def __init__(self, work_dir, table_pair, src_conn, dest_conn):
        """
        table_pair: table pair to validate
        src_conn: Database connection to the source system
        dest_conn: Database connection to the destination system
        """
        sql = "SELECT count(*) FROM %s.%s"
        src_schema = table_pair.source.schema
        src_table = table_pair.source.table
        dest_schema = table_pair.dest.schema
        dest_table = table_pair.dest.table

        TableValidator.__init__(self, work_dir, src_conn, dest_conn,
                                sql % (src_schema, src_table),
                                sql % (dest_schema, dest_table))

    @staticmethod
    def get_name():
        """
        Returns 'count'
        """
        return 'count'

# --------------------------------------------------------------------------


class MD5MergeTableValidator(TableValidator):

    """
    Validation that compares the MD5 hashes of all the rows in a table.
    """

    class MD5Sum(Command):
        """
        Prints md5sum of file.
        """

        def __init__(self, name, filename, ctxt=LOCAL, remoteHost=None):
            """
            name: command name
            filename: file to calculate md5sum of
            """
            cmdStr = "md5sum %s | awk '{print \\$1}'" % filename
            Command.__init__(self, name, cmdStr, ctxt, remoteHost)

    def __init__(self, work_dir, table_pair, src_conn, dest_conn):
        """
        table_pair: table pair to validate
        src_conn: Database connection to the source system
        dest_conn: Database connection to the destination system
        """

        sql = """COPY (SELECT md5(textin(record_out(t.*))) hash
                 FROM %s.%s t ORDER BY hash) TO '%s'"""
        src_schema = table_pair.source.schema
        src_table = table_pair.source.table
        dest_schema = table_pair.dest.schema
        dest_table = table_pair.dest.table
        self._pool = WorkerPool(2)
        self._src_pipe = os.path.join(
            work_dir, str(table_pair.source), 'src_md5_validation')
        self._dest_pipe = os.path.join(
            work_dir, str(table_pair.source), 'dest_md5_validation')

        self._src_host = DB(src_conn).host
        self._dest_host = DB(dest_conn).host

        src_pipe_cmd = GpCreateNamedPipe('Create source validation pipe',
                                         self._src_pipe,
                                         REMOTE, self._src_host)
        self._pool.addCommand(src_pipe_cmd)

        dest_pipe_cmd = GpCreateNamedPipe('Create destination validation pipe',
                                          self._dest_pipe,
                                          REMOTE, self._dest_host)
        self._pool.addCommand(dest_pipe_cmd)

        self._pool.join()
        self._pool.check_results()

        self._src_md5_cmd = MD5MergeTableValidator.MD5Sum(
            'Source MD5Sum', self._src_pipe, REMOTE, self._src_host)
        self._pool.addCommand(self._src_md5_cmd)

        self._dest_md5_cmd = MD5MergeTableValidator.MD5Sum(
            'Dest MD5Sum', self._dest_pipe, REMOTE, self._dest_host)
        self._pool.addCommand(self._dest_md5_cmd)

        TableValidator.__init__(self, work_dir, src_conn, dest_conn,
                                sql % (src_schema, src_table, self._src_pipe),
                                sql % (dest_schema, dest_table, self._dest_pipe))

    def _src_proc(self):
        """
        Thread proc that executes the SQL statement on the source side.  To
        use this method the SQL statement must only return a single row.  If
        your query requires more complexity you should override this.
        """

        try:
            self._src_res = execSQL(self._src_conn, self._src_sql)

        except:
            self._src_failed = True
        finally:
            cmd = GpCloseNamedPipe(
                'close source validation pipe', self._src_pipe, REMOTE, self._src_host)
            cmd.run()

    def _dest_proc(self):
        """
        Thread proc that executes the SQL statement on the destination side.  To
        use this method the SQL statement must only return a single row.  If
        your query requires more complexity you should override this.
        """

        try:
            self._dest_res = execSQL(self._dest_conn, self._dest_sql)
        except:
            self._dest_failed = True
        finally:
            cmd = GpCloseNamedPipe(
                'close dest validation pipe', self._dest_pipe, REMOTE, self._dest_host)
            cmd.run()

    def _compare(self):
        """
        Diff the output of the hashes
        """
        res = False
        try:
            self._pool.join()
            self._pool.check_results()
            if not self._dest_failed and not self._src_failed:
                res = ((self._src_md5_cmd.get_results()).stdout.strip()
                        == (self._dest_md5_cmd.get_results()).stdout.strip())
        finally:
            pass
        return res

    @staticmethod
    def get_name():
        """
        Returns 'md5'
        """
        return 'md5'

# --------------------------------------------------------------------------


class TableValidatorFactory(object):

    """
    Class reponsible for managing the various validation classes.
    """

    # dict of discovered validators
    __available_validators = {}

    def __init__(self):
        """
        Discovers validator classes using inpection
        """
        classes = inspect.getmembers(sys.modules[__name__],
                                     lambda m: inspect.isclass(m) and
                                     m.__module__ == __name__ and
                                     issubclass(m, TableValidator))
        for (_, c) in classes:
            name = c.get_name()
            if name:
                self.__available_validators[name] = c

    def get_available_validators(self):
        """
        Returns a list of available validator names
        """

        return TableValidatorFactory.__available_validators.keys()

    def get_validator(self, name):
        """
        Returns the class of the validator specified.

        name: name of validator to return class for
        """

        try:
            return TableValidatorFactory.__available_validators[name]
        except:
            raise Exception('Unknown validator %s' % name)

# --------------------------------------------------------------------------
# Validator factory
# --------------------------------------------------------------------------
validator_factory = TableValidatorFactory()

# --------------------------------------------------------------------------
# Command classes for data transfer
# --------------------------------------------------------------------------


class PsqlFile(Command):

    """
    psql command for executing file
    """

    def __init__(self, name, host, port, user, database, filename):
        """
        name: Command name
        host: GPDB hostname
        port: GPDB port
        user: GPDB user
        database: GPDB database
        filename: SQL file to execute
        """
        cmdStr = 'psql -U %s -h %s -p %d -f %s %s' \
            % (user, host, port, filename, database)
        Command.__init__(self, name, cmdStr, LOCAL, None)


# --------------------------------------------------------------------------

class CreateDB(Command):

    """
    Create database command
    """

    def __init__(self, name, host, port, user, database):
        """
        name: Command name
        host: GPDB hostname
        port: GPDB port
        user: GPDB user
        database: GPDB database
        """
        cmdStr = 'createdb -U %s -h %s -p %d %s' \
            % (user, host, port, database)
        Command.__init__(self, name, cmdStr, LOCAL, None)


# --------------------------------------------------------------------------

class GpCreateNamedPipe(Command):

    """
    Command for creating a named pipe on *nix systems.
    """

    def __init__(self, name, fifo_name, ctxt=LOCAL, remoteHost=None):
        """
        name: name of the command
        fifo_name: full path of the named pipe to create
        """

        cmdStr = 'mkfifo %s' % fifo_name
        Command.__init__(self, name, cmdStr, ctxt, remoteHost)


# --------------------------------------------------------------------------
class GpCloseNamedPipe(Command):

    """
    Opens and closes named pipe to make sure process at other end received
    EOF
    """

    def __init__(self, name, fifo_name, ctxt=LOCAL, remoteHost=None):
        """
        name: name of the command
        fifo_name: full path to the named pipe
        """
        cmdStr = """python -c 'f = open("%s", "w+"); f.close()'""" % fifo_name
        Command.__init__(self, name, cmdStr, ctxt, remoteHost)

# --------------------------------------------------------------------------


class GpCreateGpfdist(Command):

    """
    Command for starting a gpfdist instance.
    """

    def __init__(self, name, directory, data_file, port, last_port, max_line_length, timeout,
                 pid_file, log_file, ctxt=LOCAL, remoteHost=None):
        """
        name: name of the command
        dir: directory for gpfdist to use as its root directory
        port: port for gpfdist to listen on
        last_port: last port for gpfdist to listen on
        max_line_length: maximum line length setting for gpfdist
        pid_file: full path of the pid file to create
        """

        self._data_file = data_file

        cmdStr = \
            'nohup gpfdist -d %s -p %d -P %d -m %d -t %d > %s 2>&1 < /dev/null & echo \\$! > ' \
            '%s && bash -c "(sleep 1 && kill -0 \`cat %s 2> /dev/null\` && cat %s) || (cat %s >&2 && exit 1)"' % \
            (directory, port, last_port, max_line_length, timeout, log_file, pid_file, pid_file, log_file, log_file)

        Command.__init__(self, name, cmdStr, ctxt, remoteHost)

    def get_url(self):
        url = None
        res = self.get_results()
        if res and res.wasSuccessful():
            host = self.remoteHost
            m = re.search(GPFDIST_PORT_REGEX, res.stdout)
            if m: 
                port = m.group(1)
                url = "gpfdist://%s:%s/%s" % (host, port, self._data_file)

        if url is None:
            raise Exception("Failed to find port in gpfdist log file")
        return url

# --------------------------------------------------------------------------

class GpCleanupGpfdist(Command):

    """
    Command for terminating a running gpfdist instance and removing its pid file
    """

    def __init__(self, name, pid_file, log_file, ctxt=LOCAL, remoteHost=None):
        """
        name: name of the command
        pid_file: full path of the pid file for the gpfdist instance to
                  terminate
        """

        # We have to kill and then wait for the process to fully exit so we
        # can reuse the port.  Without this delay the next gpfdist instance
        # that tries to start using the same port can fail to bind.
        cmdStr = """python -c 'import os
import signal
import sys
import time

pid = -1

try:
    with open(sys.argv[1], "r") as f:
        pid = int(f.readline().strip())
except:
    pass

if os.path.isfile(sys.argv[2]):
    os.unlink(sys.argv[2])

if pid == -1:
    sys.exit(0)

check_count = 0

try:
    os.kill(pid, signal.SIGTERM)
except:
    pass

while True:
    try:
        os.kill(pid,0)
    except:
        break
    time.sleep(1)
    check_count += 1
    if check_count >= 10:
        try:
            os.kill(pid, signal.SIGKILL)
            while True:
                try:
                    os.kill(pid,0)
                except:
                    break
                time.sleep(1)
                check_count += 1
                if check_count >= 20:
                    os.unlink(sys.argv[1])
                    sys.exit(1)
        except:
            pass

os.unlink(sys.argv[1])' %s %s""" % (pid_file, log_file)

        Command.__init__(self, name, cmdStr, ctxt, remoteHost)


# --------------------------------------------------------------------------

class GpSchemaDump(Command):

    """
    Command for dumping the schema of a GPDB table.  Currently it only dumps
    the table schema and does not include ownership or dependent database
    objects.
    """

    def __init__(self, name, host, port, user, full=True, database=None,
                 schema=None, table=None, ctxt=LOCAL, remoteHost=None):
        """
        name: name of the command
        host: GPDB host
        port: GPDB port
        user: GPDB user (must be admin account or table owner)
        full: Full dump of system
        database: database containing table to dump
        schema: schema containing table to dump
        table: table to dump
        """
        cmdStr = None
        self.full = full

        if not full and (database is None or schema is None or table is None):
            raise Exception('database, schema and table are required for '
                            'table dump')

        if full:
            cmdStr = 'pg_dumpall -s --gp-syntax -h %s -p %d -U %s' \
                % (host, port, user)
        else:
            cmdStr = 'pg_dump -s -x -O --gp-syntax -h %s -p %d -U %s -t ' \
                '\'\"%s\".\"%s\"\' %s' % (host, port, user, schema, table, database)

        Command.__init__(self, name, cmdStr, ctxt, remoteHost)

    def run(self):
        """
        Runs the command.
        """

        Command.run(self)

    def get_schema_sql(self):
        """
        Returns a string containing the SQL statement to create the table.
        """

        # Look for the start of the dump text to exclude anything like a MOTD
        sql = None
        res = self.get_results()
        if res is None:
            raise Exception('Command has not been run')

        if self.full:
            sql = res.stdout
        else:
            start_text = """--
-- Greenplum Database database dump
--"""
            start_pos = res.stdout.find(start_text)
            if start_pos == -1:
                return None
            sql = res.stdout[start_pos:]

        return sql


# --------------------------------------------------------------------------
class GpTransferCommand(Command):

    """
    Command to transfer data in a table from one GPDB system to another.

    TODO: too many params
    """

    def __init__(
        self, name, src_host, src_port, src_user, dest_host, dest_port,
        dest_user, table_pair, dest_exists, truncate, analyze, drop,
        fast_mode, exclusive_lock, schema_only, work_dir,
        host_map, source_config, batch_size, gpfdist_port, gpfdist_last_port, 
        gpfdist_instance_count, max_line_length, timeout, wait_time, 
        delimiter, validator, format, quote, table_transfer_set_total):
        """
        name: name of the command
        src_host: source GPDB host
        src_port: source GPDB port
        src_user: source GPDB user
        dest_host: destination GPDB host
        dest_port: destination GPDB port
        dest_user: destination GPDB user
        table_pair: description of source -> dest table transfer
        dest_exists: if the destination table exists already
        truncate: if the destination table should be truncated
        analyze: if the destination table should be analyzed after transfer
        drop: if the destination table should be dropped
        fast_mode: should fast mode of operation be used
        exclusive_lock: exclusive lock the source table
        schema_only: only create table
        work_dir: the work directory to create named pipes in
        host_map: the host to ip mapping of the source system
        source_config: the GpArray of the source GPDB system
        batch_size: the size of the WorkerPool for the command
        gpfdist_port: gpfdist port
        gpfdist_last_port: last gpfdist port
        gpfdist_instance_count: gpfdist instances per source host
        max_line_length: the gpfdist maximum line length
        timeout: the gpfdist timeout
        wait_time: time to wait on destination query (TODO: remove this in next release)
        delimiter: delimiter char to use for external tables
        validator: validator to use
        format: transfer data in CSV(default) or TEXT format
        quote: specifies the quotation character for CSV mode
        table_transfer_set_total: Number of tables need to be transferred
        """

        self._src_host = src_host
        self._src_port = src_port
        self._src_user = src_user
        self._dest_host = dest_host
        self._dest_port = dest_port
        self._dest_user = dest_user
        self._table_pair = table_pair
        self._dest_exists = dest_exists
        self._truncate = truncate
        self._analyze = analyze
        self._drop = drop
        self._fast_mode = fast_mode
        self._exclusive_lock = exclusive_lock
        self._schema_only = schema_only
        self._work_dir = work_dir
        self._host_map = host_map
        self._source_config = source_config
        self._batch_size = batch_size
        self._gpfdist_port = gpfdist_port
        self._gpfdist_last_port = gpfdist_last_port
        self._gpfdist_instance_count = gpfdist_instance_count
        self._max_line_length = max_line_length
        self._timeout = timeout
        self._wait_time = wait_time
        self._delimiter = delimiter
        self._format = format
        self._quote = quote
        self._table_transfer_set_total = table_transfer_set_total
        # _used_ports is a dict where key is hostname and val is list of
        # ints (ports used)
        self._pipe = os.path.join(work_dir,
                                  str(table_pair.source),
                                  '%s.pipe' % str(table_pair.source))
        self._wext_gpfdist_urls = list()
        self._ext_gpfdist_urls = list()
        self._wext_name = ('w_ext_%s_%s' % (self._table_pair.source.table,
                                            hashlib.md5(str(self._table_pair.source)).hexdigest()))[0:63]
        self._ext_name = ('ext_%s_%s' % (self._table_pair.source.table,
                                         hashlib.md5(str(self._table_pair.source)).hexdigest()))[0:63]
        self._pool = None
        if validator:
            self._validator_class = validator_factory.get_validator(validator)
        else:
            self._validator_class = None

        # connections saved off so we can cancel executing SQL from outside
        # the thread executing the SQL statement
        self._src_conn = None
        self._dest_conn = None
        self._src_failed = False
        self._dest_failed = False
        self._status_msg = 'Success'
        self._src_exception = None
        self._dest_exception = None
        self._success = False
        self._failed_to_stop_gpfdists = False
        self._cleanup_gpfdist_needed = False
        self._cleanup_named_pipes_needed = False
        self._src_ready = Event()
        self._dest_ready = Event()

        Command.__init__(self, name, None, LOCAL, None)

    def run(self):
        """
        Runs the full table transfer, executing all steps needed.
        """
        global tableCountLock
        global remaining_tables

        try:
            if not canceled:
                self._pool = WorkerPool(self._batch_size)
                logger.info('Starting transfer of %s to %s...',
                            self._table_pair.source,
                            self._table_pair.dest)

                url = DbURL(self._src_host, self._src_port,
                            self._table_pair.source.database, self._src_user)
                self._src_conn = connect(url)

                url = DbURL(self._dest_host, self._dest_port,
                            self._table_pair.dest.database, self._dest_user)
                self._dest_conn = connect(url)

                if not self._dest_exists:
                    self._create_target_table()
                elif self._truncate and not self._table_pair.dest.external:
                    self._truncate_target_table()
                elif self._drop:
                    self._drop_target_table()
                    self._create_target_table()
                if self._schema_only:
                    return
                if not self._table_pair.dest.external:
                    self._cleanup_named_pipes_needed = True
                    self._create_named_pipe()
                    self._cleanup_gpfdist_needed = True
                    self._start_write_gpfdist()
                    self._start_read_gpfdist()
                    self._create_source_wext()
                    self._create_dest_ext()
                    self._transfer_data()
                    if self._validator_class:
                        self._validate()
                    if self._analyze:
                        self._analyze_dest_table()
                    self._cleanup_dest_ext()
                    self._cleanup_source_wext()
            else:
                self.set_results(
                    CommandResult(1, 'Canceled', None, False, False))
        except ExecutionError, exerr:
            self.set_results(exerr.cmd.get_results())
        except Exception, ex:
            self.set_results(CommandResult(1, str(ex), None, False, False))
        else:
            self._success = True
        finally:
            if self._cleanup_gpfdist_needed:
                try:
                    self._cleanup_gpfdist()
                except:
                    global running_gpfdists
                    running_gpfdists = True

            if self._cleanup_named_pipes_needed:
                try:
                    self._cleanup_named_pipe()
                except:
                    pass  # will be cleaned up at the end

            if self._src_conn:
                self._src_conn.commit(
                ) if self._success else self._src_conn.rollback()
                self._src_conn.close()

            if self._dest_conn:
                self._dest_conn.commit(
                ) if self._success else self._dest_conn.rollback()
                self._dest_conn.close()

            if self._pool:
                self._pool.haltWork()
                self._pool.joinWorkers()

            if not self.get_results():
                self.set_results(
                    CommandResult(0, 'Success', None, True, False))

            if not canceled:
                if self._success:
            	    with tableCountLock:
                    	remaining_tables = remaining_tables - 1
                    logger.info(
                               "Finished transferring table %s, remaining %s of %s tables", str(self._table_pair.source), remaining_tables, self._table_transfer_set_total)
                else:
                    logger.error(
                        "Failed to transfer table %s", str(self._table_pair.source))
                    if self._src_exception:
                        logger.error(self._src_exception)
                    if self._dest_exception:
                        logger.error(self._dest_exception)
                    logger.info('Remaining %s of %s tables', remaining_tables, self._table_transfer_set_total)


    def _get_distributed_by(self):
        sql = '''SELECT STRING_AGG(attname, ', ' ORDER BY colorder)
FROM (
  SELECT localoid, UNNEST(attrnums) AS colnum, GENERATE_SERIES(1, ARRAY_UPPER(attrnums, 1)) AS colorder
  FROM gp_distribution_policy
) d
JOIN pg_attribute a ON (d.localoid = a.attrelid AND d.colnum = a.attnum)
JOIN pg_class c ON (d.localoid = c.oid)
JOIN pg_namespace n ON (c.relnamespace = n.oid)
WHERE nspname = '%s'
AND relname = '%s'
GROUP BY nspname, relname;''' % (self._table_pair.source.schema,
                                 self._table_pair.source.table)
        try:
            row = execSQLForSingletonRow(self._src_conn, sql)
            return 'DISTRIBUTED BY (%s)' % row[0]
        except:
            return 'DISTRIBUTED RANDOMLY'

    def _create_target_table(self):
        """
        Creates the table on the target GPDB system.
        """
        logger.info('Creating target table %s...', self._table_pair.dest)
        schema_sql = self._get_source_table_schema()
        cur = execSQL(self._dest_conn, schema_sql)
        cur.close()

    def _truncate_target_table(self):
        """
        Truncates the table on the target GPDB system.
        """

        logger.info('Truncating target table %s...', self._table_pair.dest)
        cur = execSQL(self._dest_conn, 'TRUNCATE TABLE \"%s\".\"%s\"'
                      % (self._table_pair.dest.schema,
                         self._table_pair.dest.table))
        cur.close()

    def _drop_target_table(self):
        """
        Drops the table on the target GPDB system.
        """

        logger.info('Dropping target table %s...', self._table_pair.dest)
        cur = execSQL(self._dest_conn, 'DROP TABLE \"%s\".\"%s\"'
                      % (self._table_pair.dest.schema,
                         self._table_pair.dest.table))
        cur.close()

    def _get_source_table_schema(self):
        """
        Gets the SQL to create the table from the source GPDB system.
        """

        self._pool.empty_completed_items()
        logger.info('Retrieving schema for table %s...',
                    self._table_pair.source)
        cmd = GpSchemaDump(
            'schema dump of %s' % self._table_pair.source,
            self._src_host,
            self._src_port,
            self._src_user,
            False,
            self._table_pair.source.database,
            self._table_pair.source.schema,
            self._table_pair.source.table,
            ctxt=REMOTE,
            remoteHost=self._src_host
        )
        self._pool.addCommand(cmd)
        self._pool.join()
        self._pool.check_results()
        return cmd.get_schema_sql()

    def _get_named_pipes(self):
        """
        """
        named_pipes = list()
        if self._fast_mode:
            for seg in self._source_config.getSegDbList():
                if seg.isSegmentMirror(True) or seg.isSegmentQD():
                    continue
                pipe = "%s.%d" % (self._pipe, seg.getSegmentContentId())
                address = iter(self._host_map[seg.getSegmentHostName()]).next()
                named_pipes.append((address, pipe))
        else:
            for host in self._host_map.keys():
                address = iter(self._host_map[host]).next()
                for i in xrange(0, self._gpfdist_instance_count):
                    pipe = "%s.%d" % (self._pipe, i)
                    named_pipes.append((address, pipe))
        return named_pipes        

    def _create_named_pipe(self):
        """
        Creates all the named pipes needed for the table transfer on the
        source GPDB system.
        """

        self._pool.empty_completed_items()
        logger.debug('Creating FIFO pipes for source table %s...',
                    self._table_pair.source)

        for host in self._host_map.keys():
            address = iter(self._host_map[host]).next()
            cmd = MakeDirectory('create dir for table %s'
                                % self._table_pair.source,
                                os.path.dirname(self._pipe), REMOTE, address)
            self._pool.addCommand(cmd)

        cmd = MakeDirectory('create dir for table %s'
                            % self._table_pair.source,
                            os.path.dirname(self._pipe), REMOTE, self._src_host)
        self._pool.addCommand(cmd)
        cmd = MakeDirectory('create dir for table %s'
                            % self._table_pair.source,
                            os.path.dirname(self._pipe), REMOTE, self._dest_host)
        self._pool.addCommand(cmd)

        self._pool.join()
        self._pool.check_results()


        for (address, pipe) in self._get_named_pipes():
            cmd = GpCreateNamedPipe('Create pipe for table %s on %s'
                                    % (self._table_pair.source, address), 
                                    pipe, REMOTE, address)

            self._pool.addCommand(cmd)
        self._pool.join()
        self._pool.check_results()

    def _create_source_wext(self):
        """
        Creates the writable external table on the source GPDB system.
        """

        logger.debug('Creating source writable external table for source '
                    'table %s...', self._table_pair.source)

        urls = ','.join(["'%s'" % url for url in self._wext_gpfdist_urls])

        if self._fast_mode:
            distributed_clause = self._get_distributed_by()
            wext_sql = \
                '''CREATE WRITABLE EXTERNAL WEB TABLE gptransfer.%s (LIKE \"%s\".\"%s\")
                   EXECUTE 'cat > %s.$GP_SEGMENT_ID'
                   FORMAT '%s' 
                ''' % (self._wext_name,
                       self._table_pair.source.schema,
                       self._table_pair.source.table,
                       self._pipe,
                       self._format)
            if self._format.lower() == 'csv':
                wext_sql += """ (DELIMITER AS ',' QUOTE AS E'%s')""" % self._quote
            elif self._format.lower() == 'text':
                wext_sql += """ (DELIMITER AS E'%s' ESCAPE AS 'off')""" % self._delimiter
            wext_sql += """ ENCODING 'UTF8' %s""" % distributed_clause
        else:
            wext_sql = \
                '''CREATE WRITABLE EXTERNAL TABLE gptransfer.%s ( LIKE \"%s\".\"%s\")
                   LOCATION (%s)
                   FORMAT '%s' ''' \
                % (self._wext_name,
                   self._table_pair.source.schema,
                   self._table_pair.source.table,
                   urls,
                   self._format)
            if self._format.lower() == 'csv':
                wext_sql += """ (DELIMITER AS ',' QUOTE AS E'%s')""" % self._quote
            elif self._format.lower() == 'text':
                wext_sql += """ (DELIMITER AS E'%s' ESCAPE AS 'off')""" % self._delimiter
            wext_sql += """ ENCODING \'UTF8\' DISTRIBUTED RANDOMLY"""
        cur = execSQL(self._src_conn, wext_sql)
        cur.close()

    def _create_dest_ext(self):
        """
        Creates the readable external table on the destination GPDB system.
        """

        logger.debug('Creating external table for destination table %s...',
                    self._table_pair.dest)
        urls = ','.join(["'%s'" % url for url in self._ext_gpfdist_urls])
        ext_sql = \
            """CREATE EXTERNAL TABLE gptransfer.%s (LIKE \"%s\".\"%s\")
               LOCATION(%s)
               FORMAT '%s' """\
            % (self._ext_name,
               self._table_pair.dest.schema,
               self._table_pair.dest.table,
               urls,
               self._format)
        if self._format.lower() == 'csv':
            ext_sql += """(DELIMITER AS ',' QUOTE AS E'%s')""" % self._quote
        elif self._format.lower() == 'text':
            ext_sql += """(DELIMITER AS E'%s' ESCAPE AS 'off')""" % self._delimiter
        cur = execSQL(self._dest_conn, ext_sql)
        cur.close()

    def _analyze_dest_table(self):
        """
        Analyzes destination table
        """

        logger.info('Analyzing destination table %s...', self._table_pair.dest)
        sql = 'ANALYZE \"%s\".\"%s\"' % (self._table_pair.dest.schema,
                                 self._table_pair.dest.table)
        cur = execSQL(self._dest_conn, sql)
        cur.close()

    def _start_write_gpfdist(self):
        """
        Starts gpfdist instances for write
        """
        self._pool.empty_completed_items()
        if self._fast_mode:
            return
        else:
            logger.info(
                'Starting gpfdist for writable external table for table %s...',
                self._table_pair.source)
            for host in self._host_map.keys():
                address = iter(self._host_map[host]).next()

                for i in xrange(0, self._gpfdist_instance_count):
                    cmd = GpCreateGpfdist(
                        'gpfdist for table %s on %s' % (self._table_pair.source,
                                                        address),
                        os.path.dirname(self._pipe),
                        "%s.%d" % (os.path.basename(self._pipe), i),
                        self._gpfdist_port,
                        self._gpfdist_last_port,
                        self._max_line_length,
                        self._timeout,
                        os.path.join(self._work_dir, 'gpfdist_write_%s_%d.pid'
                                     % (str(self._table_pair.source), i)),
                        os.path.join(self._work_dir, 'gpfdist_write_%s_%d.log'
                                     % (str(self._table_pair.source), i)),
                        REMOTE,
                        address
                    )
                    self._pool.addCommand(cmd)

            self._pool.join()
            for cmd in self._pool.getCompletedItems():
                if not cmd.get_results().wasSuccessful():
                    raise  ExecutionError("Error Executing Command: ", cmd)           
                url = cmd.get_url()
                self._wext_gpfdist_urls.append(url)

            self._pool.empty_completed_items()

    def _start_read_gpfdist(self):
        """
        Starts gpfdist instances for read
        """
        self._pool.empty_completed_items()

        logger.debug(
            'Starting gpfdist for readable external table for table %s...',
            self._table_pair.source)

        if self._fast_mode:
            for seg in self._source_config.getSegDbList():
                if seg.isSegmentMirror(True) or seg.isSegmentQD():
                    continue
                address = iter(self._host_map[seg.getSegmentHostName()]).next()
                cmd = GpCreateGpfdist(
                    'gpfdist for table %s on %s' % (self._table_pair.source,
                                                    seg.getSegmentHostName()),
                    os.path.dirname(self._pipe),
                    "%s.%d" % (os.path.basename(self._pipe), seg.getSegmentContentId()),
                    self._gpfdist_port,
                    self._gpfdist_last_port,
                    self._max_line_length,
                    self._timeout,
                    os.path.join(self._work_dir, 'gpfdist_read_%s_%d.pid'
                                 % (str(self._table_pair.source), seg.getSegmentContentId())),
                    os.path.join(self._work_dir, 'gpfdist_read_%s_%d.log'
                                 % (str(self._table_pair.source), seg.getSegmentContentId())),
                    REMOTE,
                    address)
                self._pool.addCommand(cmd)
        else:
            for host in self._host_map.keys():
                address = iter(self._host_map[host]).next()

                for i in xrange(0, self._gpfdist_instance_count):
                    cmd = GpCreateGpfdist(
                        'gpfdist for table %s on %s' % (self._table_pair.source,
                                                        address),
                        os.path.dirname(self._pipe),
                        "%s.%d" % (os.path.basename(self._pipe), i),
                        self._gpfdist_port,
                        self._gpfdist_last_port,
                        self._max_line_length,
                        self._timeout,
                        os.path.join(self._work_dir, 'gpfdist_read_%s_%d.pid'
                                     % (str(self._table_pair.source), i)),
                        os.path.join(self._work_dir, 'gpfdist_read_%s_%d.log'
                                     % (str(self._table_pair.source), i)),
                        REMOTE,
                        address,
                    )
                    self._pool.addCommand(cmd)
        self._pool.join()

        for cmd in self._pool.getCompletedItems():
            if not cmd.get_results().wasSuccessful():
                raise  ExecutionError("Error Executing Command: ", cmd)           
            url = cmd.get_url()
            self._ext_gpfdist_urls.append(url)

        self._pool.empty_completed_items()


    def _transfer_data(self):
        """
        Moves data from source to destination GPDB system.
        """
        self._pool.empty_completed_items()

        logger.info('Transfering data %s -> %s...',
                    self._table_pair.source, self._table_pair.dest)
        source_thread = Thread(target=self._do_wext_select_proc)
        source_thread.start()
        dest_thread = Thread(target=self._do_ext_insert_select_proc)
        dest_thread.start()

        while True:
            time.sleep(1)
            if not source_thread.isAlive():
                if self._src_failed == True:
                    DB(self._dest_conn).cancel()
                else:
                    # Make sure all named pipes get an EOF on them or empty tables
                    # will hang.
                    # TODO: currently no way to ensure gpfdist on the write side has
                    #       written all data available to the pipe.  Closing the pipe
                    #       while data is remaining causes and EPIPE on the writable
                    #       gpfdist side.  Need a better way to coordinate this.
                    time.sleep(self._wait_time)
                    logger.debug('Closing named pipes for table %s...', str(self._table_pair.source))
                    for (address, pipe) in self._get_named_pipes():
                        cmd = GpCloseNamedPipe('close pipe %s on %s' % (pipe, address),
                                               pipe, REMOTE, address)
                        self._pool.addCommand(cmd)

                    self._pool.join()
                    self._pool.check_results()
                break

            if not dest_thread.isAlive():
                if self._dest_failed == True:
                    DB(self._src_conn).cancel()

            if canceled == True:
                try:
                    logger.info('Canceling source query...')
                    DB(self._src_conn).cancel()
                    source_thread.join()
                except Exception, ex:
                    logger.info(
                        'Exception while canceling source query: %s', str(ex))

                try:
                    logger.info('Canceling destination query...')
                    DB(self._dest_conn).cancel()
                    dest_thread.join()
                except Exception, ex:
                    logger.info(
                        'Exception while canceling destination query: %s', str(ex))

                break

        if source_thread.isAlive():
            source_thread.join()
        if dest_thread.isAlive():
            dest_thread.join()

        if self._src_failed or self._dest_failed:
            raise Exception('Failed to transfer data')

        return True

    def _do_wext_select_proc(self):
        """
        Thread proc for the source side of the transfer.
        """
        cur = None
        try:
            query = """INSERT INTO gptransfer.%s SELECT * FROM \"%s\".\"%s\"""" \
                % (self._wext_name,
                   self._table_pair.source.schema,
                   self._table_pair.source.table)
            if self._exclusive_lock:
                lock_query = "LOCK TABLE \"%s\".\"%s\" IN EXCLUSIVE MODE;" % \
                    (self._table_pair.source.schema, self._table_pair.source.table)
                execSQL(self._src_conn, lock_query)

            self._src_ready.set()
            wait_until_set_or_cancel(self._dest_ready)
            if canceled:
                return

            cur = execSQL(self._src_conn, query)
        except Exception, ex:
            self._src_failed = True
            self._src_exception = ex
        finally:
            if cur:
                cur.close()
            thread.exit()

    def _do_ext_insert_select_proc(self):
        """
        Thread proc for the destination side of the transfer.
        """
        cur = None
        try:
            query = """INSERT INTO \"%s\".\"%s\" SELECT * FROM gptransfer.%s""" \
                % (self._table_pair.dest.schema,
                   self._table_pair.dest.table,
                   self._ext_name)
            if self._exclusive_lock:
                lock_query = "LOCK TABLE \"%s\".\"%s\" IN EXCLUSIVE MODE" % \
                    (self._table_pair.dest.schema, self._table_pair.dest.table)
                execSQL(self._dest_conn, lock_query)

            self._dest_ready.set()
            wait_until_set_or_cancel(self._src_ready)
            if canceled:
                return

            # TODO: The source query should to start executing before the destination
            #       query but there is no way to coordinate the two here.
            time.sleep(1)
            cur = execSQL(self._dest_conn, query)
            cur.close()
        except Exception, ex:
            self._dest_failed = True
            self._dest_exception = ex
        finally:
            if cur:
                cur.close()
            thread.exit()

    def _cleanup_source_wext(self):
        """
        Cleans up the writable external table on the source GPDB system.
        """

        try:
            logger.debug(
                'Dropping writable external table for source table %s...',
                self._table_pair.source)
            drop_wext_sql = """DROP EXTERNAL WEB TABLE gptransfer.%s""" \
                % (self._wext_name)
            cur = execSQL(self._src_conn, drop_wext_sql)
            cur.close()
        except:
            # We'll drop at the end when we wipe out the gptransfer schema
            pass

    def _cleanup_dest_ext(self):
        """
        Cleans up the readable external table on the destination GPDB system.
        """

        try:
            logger.debug('Dropping external table for destination table %s...',
                        self._table_pair.dest)
            drop_ext_sql = """DROP EXTERNAL TABLE gptransfer.%s""" \
                % (self._ext_name)
            cur = execSQL(self._dest_conn, drop_ext_sql)
            cur.close()
        except:
            # We'll drop at the end when we wipe out the gptransfer schema
            pass

    def _cleanup_gpfdist(self):
        """
        Stops the gpfdist instances on the source GPDB system that were started
        for this table transfer.
        """

        self._pool.empty_completed_items()
        logger.debug('Stopping gpfdist for source table %s...',
                    self._table_pair.source)
        if self._fast_mode:
            for seg in self._source_config.getSegDbList():
                if seg.isSegmentMirror(True) or seg.isSegmentQD():
                    continue
                address = iter(self._host_map[seg.getSegmentHostName()]).next()
                cmd = GpCleanupGpfdist('stopping gpfdist on %s' % address,
                                       os.path.join(self._work_dir, 'gpfdist_read_%s_%d.pid' % (
                                           str(self._table_pair.source), seg.getSegmentContentId())),
                                       os.path.join(self._work_dir, 'gpfdist_read_%s_%d.log' % (
                                           str(self._table_pair.source), seg.getSegmentContentId())),
                                       REMOTE, address)
                self._pool.addCommand(cmd)
        else:
            for host in self._host_map.keys():
                address = iter(self._host_map[host]).next()

                for i in xrange(0, self._gpfdist_instance_count):
                    cmd = GpCleanupGpfdist('stopping gpfdist on %s' % address,
                                           os.path.join(self._work_dir,
                                           'gpfdist_read_%s_%d.pid' % (str(self._table_pair.source), i)),
                                           os.path.join(self._work_dir,
                                           'gpfdist_read_%s_%d.log' % (str(self._table_pair.source), i)),
                                           REMOTE, address)
                    self._pool.addCommand(cmd)
                    cmd = GpCleanupGpfdist(
                        'stopping gpfdist on %s' % address,
                        os.path.join(self._work_dir,
                        'gpfdist_write_%s_%d.pid' % (str(self._table_pair.source), i)),
                        os.path.join(self._work_dir,
                        'gpfdist_write_%s_%d.log' % (str(self._table_pair.source), i)),
                        REMOTE, address)
                    self._pool.addCommand(cmd)

        self._pool.join()
        self._pool.check_results()

    def _cleanup_named_pipe(self):
        """
        Deletes the named pipes on the source GPDB system that were created for
        this table transfer.
        """

        self._pool.empty_completed_items()
        logger.debug('Removing FIFO pipes for source table %s...',
                    self._table_pair.source)

        for host in self._host_map.keys():
            address = iter(self._host_map[host]).next()
            cmd = RemoveFiles('remove dir for table %s'
                              % self._table_pair.source,
                              os.path.dirname(self._pipe),
                              REMOTE, address)
            self._pool.addCommand(cmd)

        cmd = RemoveFiles('remove dir for table %s'
                          % self._table_pair.source,
                          os.path.dirname(self._pipe),
                          REMOTE, self._dest_host)
        self._pool.addCommand(cmd)
        cmd = RemoveFiles('remove dir for table %s'
                          % self._table_pair.source,
                          os.path.dirname(self._pipe),
                          REMOTE, self._src_host)
        self._pool.addCommand(cmd)

        self._pool.join()
        self._pool.check_results()


    def _validate(self):
        """
        Validates the table transfer.
        """

        logger.info(
            'Validating destination table %s...', self._table_pair.dest)

        validator = self._validator_class(self._work_dir,
                                          self._table_pair,
                                          self._src_conn,
                                          self._dest_conn)
        if not validator.validate():
            logger.error('Validation failed for %s', self._table_pair.dest)
            self._success = False
            self._status_msg = 'Validation failed'
        else:
            logger.info('Validation of %s successful', self._table_pair.dest)
            self._success = True

    def get_table_pair(self):
        """
        Returns table pair for this transfer task
        """
        return self._table_pair


# --------------------------------------------------------------------------

class GpTransferTable(object):

    """
    Class for describing one side of the data transfer.
    """

    def __init__(self, database, schema, table, external):
        """
        database: database name
        schema: schema name
        table: table name
        """

        assert database is not None
        assert schema is not None
        assert table is not None
        (self.database, self.schema, self.table) = (database, schema,
                                                    table)
        self.external = external

    def __str__(self):
        """
        Fully qualified string representation of the table.
        """

        return '%s.%s.%s' % (self.database, self.schema, self.table)

    def __eq__(self, other):
        """
        Equality operator.
        """

        return self.__dict__ == other.__dict__

    def __hash__(self):
        """
        Hash value of the GpTransferTable
        """

        return hash(str(self))


class GpTransferTablePair(object):

    """
    Wrapper the describes the source and destination table transfer.
    """

    def __init__(self, source, dest):
        """
        source: the source GpTransferTable
        dest: the destination GpTransferTable
        """

        assert source is not None
        assert dest is not None
        self.source = source
        self.dest = dest

    def __str__(self):
        """
        String representation of the transfer pair.
        """

        return '%s -> %s' % (self.source, self.dest)

    def __eq__(self, other):
        """
        Equality operator.

        other: obj to compare to
        """

        return self.source == other.source and self.dest == other.dest

    def __hash__(self):
        """
        Hash value of transfer pair.
        """

        return hash(str(self))


# --------------------------------------------------------------------------
#
# --------------------------------------------------------------------------

class GpTransfer(object):

    """
    Main application class for data transfer.
    """

    def __init__(self, options, _args):  # pylint: disable=W0613
        """
        options: options dict from the options parser.
        _args: args from the options parser.  Not used.
        """
        self._options = options
        self._work_dir = os.path.join(self._options.work_base_dir,
                                      GPTRANSFER_TMP_DIR)

        self._cleanup_schemas = False
        self._excluding_table = True

        # --format would be 'TEXT' if delimiter is other than ','
        if self._options.delimiter != ",":
            self._options.format = 'text'

        # get GpArray objects of source and destination GPDB systems
        self._src_config = None
        self._dest_config = None
        self._get_configurations()

        self._src_databases = None
        self._dest_databases = None

        self._all_src_databases = get_user_databases(self._options.source_host,
                                             self._options.source_port,
                                             self._options.source_user)
        self._all_dest_databases = get_user_databases(self._options.dest_host,
                                             self._options.dest_port,
                                             self._options.dest_user)

        self._src_tables = self._get_source_tables()

        self._validate_schema_not_exists()

        # get all the tables from the destination GPDB system
        logger.info("Retrieving list of destination tables...")
        self._dest_tables = \
            get_user_tables(self._options.dest_host,
                            self._options.dest_port,
                            self._options.dest_user,
                            [db for db in self._dest_databases if db in self._all_dest_databases])

        # segment address list
        self._host_map = self._get_host_map()

        # build up table pairs to transfer and validate them
        self._table_transfer_set = self._build_table_transfer_set()
        self._table_transfer_set_total = len(self._table_transfer_set)

        self._fast_mode = \
            True if (self._dest_config.numPrimarySegments >= self._src_config.numPrimarySegments and
                     not options.force_standard_mode) \
            else False
        if self._fast_mode:
            logger.info('gptransfer will use "fast" mode for transfer.')
        else:
            logger.info('gptransfer will use "standard" mode for transfer.')

        if self._options.exclusive_lock:
            logger.info("Exclusive locks will be used during table transfers.")

        # Get number of gpfdists per source host
        source_host_count = len(self._host_map.keys())
        self._gpfdist_instance_count = \
            min(self._dest_config.numPrimarySegments / source_host_count,
                self._options.max_gpfdist_instances)

        # validate options
        self._validate_options()

        self._validate_host_map()

        self._validate_table_transfer_set()

        # At most we'll execute primary segment count of commands in the subbatches
        # so we can reduce it if the value exceeds the number of primary segs.
        if self._src_config.get_primary_count() < self._options.sub_batch_size \
                and not self._options.enable_test:
            self._options.sub_batch_size = self._src_config.get_primary_count()
        logger.info('Using batch size of %d', self._options.batch_size)
        logger.info('Using sub-batch size of %d', self._options.sub_batch_size)

        self._pool = WorkerPool(self._options.batch_size)

    def run(self):
        """
        Runs the data transfer based on the options provided.
        """
        global remaining_tables
        remaining_tables = len(self._table_transfer_set)
        if not self._options.dry_run:
            self.setup()

            if self._options.full and self._options.schema_only:
                # we are done if full and schema only
                return 0

            # Hacky, but if we don't set num_assigned then isDone() will never
            # be done.
            self._pool.empty_completed_items()
            assert self._pool.work_queue.qsize() == 0
            self._pool.num_assigned = 0

            # Create all the commands to do the transfers
            # To avoid OOM errors with a large number of tables
            # we add commands in batches
            failed_tables = list()
            max_queued = self._options.batch_size + 1
            cmds_queued = 0
            for table_pair in self._table_transfer_set:
                truncate = (True if self._options.truncate
                            and table_pair.dest
                            in self._dest_tables else False)
                drop = (True if self._options.drop
                        and table_pair.dest
                        in self._dest_tables else False)

                cmd = GpTransferCommand(
                    'transfer of %s' % table_pair.source,
                    self._options.source_host,
                    self._options.source_port,
                    self._options.source_user,
                    self._options.dest_host,
                    self._options.dest_port,
                    self._options.dest_user,
                    table_pair,
                    True if self._options.full else
                    table_pair.dest in self._dest_tables,
                    truncate,
                    self._options.analyze,
                    drop,
                    self._fast_mode,
                    self._options.exclusive_lock,
                    self._options.schema_only,
                    self._work_dir,
                    self._host_map,
                    self._src_config,
                    self._options.sub_batch_size,
                    self._options.base_port,
                    self._options.last_port,
                    self._gpfdist_instance_count,
                    self._options.max_line_length,
                    self._options.timeout,
                    self._options.wait_time,
                    self._options.delimiter,
                    self._options.validator,
                    self._options.format,
                    self._options.quote,
                    self._table_transfer_set_total
                )
                self._pool.addCommand(cmd)
                cmds_queued += 1

                (successful_commands, failed_commands) = wait_for_pool(self._pool, 
                                                                       max_queued,
                                                                       cmds_queued == len(self._table_transfer_set))
                for failed_cmd in failed_commands:
                    failed_tables.append(failed_cmd.get_table_pair().source)

            if len(failed_tables) > 0:
                with open(GPTRANSFER_FAILED_TABLES_FILE, 'w') as failed_file:
                    for table in failed_tables:
                        failed_file.write("%s%s%s%s%s\n" %
                                         (table.database, SCHEMA_DELIMITER, table.schema, SCHEMA_DELIMITER, table.table))
                logger.warn(
                    '%s tables failed to transfer.  A list of these tables', len(failed_tables))
                logger.warn(
                    'has been written to the file %s', GPTRANSFER_FAILED_TABLES_FILE)
                logger.warn(
                    'This file can be used with the -f option to continue')
                logger.warn('the data transfer.')
                return 1
        else:
            # dry run
            self._dry_run()

        return 0

    def setup(self):
        """
        Creates work directories, created needed databases, tables, etc., and
        initializes the validator if needed.
        """
        # create base pipe directory on each source system
        logger.info('Creating work directory...')

        for host in self._host_map.keys():
            address = iter(self._host_map[host]).next()
            cmd = MakeDirectory('Create work directory on %s' % host,
                                self._work_dir, REMOTE, address)
            self._pool.addCommand(cmd)

        cmd = MakeDirectory('Create work directory',
                            self._work_dir, REMOTE, self._options.source_host)
        self._pool.addCommand(cmd)

        cmd = MakeDirectory('Create work directory',
                            self._work_dir, REMOTE, self._options.dest_host)
        self._pool.addCommand(cmd)

        self._pool.join()
        self._pool.check_results()

        # if transferring entire system, dumpall and execute on destination
        if self._options.full:
            logger.info('Restoring full schema...')
            dump_schema_cmd = GpSchemaDump('dump full schema',
                                           '127.0.0.1',
                                           self._options.source_port,
                                           self._options.source_user,
                                           ctxt=REMOTE,
                                           remoteHost=self._options.source_host)
            dump_schema_cmd.run()
            res = dump_schema_cmd.get_results()
            if not res or not res.wasSuccessful():
                raise ExecutionError('Error Executing Command: ', dump_schema_cmd)
            schema_sql = dump_schema_cmd.get_schema_sql()

            schema_filename = os.path.join(self._work_dir, 'full_schema.sql')
            with open(schema_filename, 'w') as full_schema_file:
                full_schema_file.write(schema_sql)

            psql_cmd = PsqlFile('Create full schema',
                                self._options.dest_host,
                                self._options.dest_port,
                                self._options.dest_user,
                                'template1',
                                schema_filename)
            self._pool.addCommand(psql_cmd)
            self._pool.join()
            self._pool.check_results()
            os.unlink(schema_filename)
            if os.path.isfile(schema_filename):
                logger.warn(
                    'Failed to remove schema file %s.', schema_filename)
                logger.warn('This file should be removed manually.')

            # Restoring the full schema uses template0 as the template database
            # This means that while the gp_toolkit schema is created in each database
            # it is not populated.  Because of this we need to populate it ourselves.
            url = DbURL(self._options.dest_host, self._options.dest_port,
                        'template1', self._options.dest_user)
            conn = connect(url)
            databases = [db[0] for db in getUserDatabaseList(conn)]
            conn.close()

            toolkit_file = os.path.join(self._gphome, 'share', 'postgresql', 'gp_toolkit.sql')
            logger.info('Populating gp_toolkit schema...')
            for database in databases:
                toolkit_cmd = PsqlFile('Create gptoolkit',
                                self._options.dest_host,
                                self._options.dest_port,
                                self._options.dest_user,
                                database,
                                toolkit_file)
                self._pool.addCommand(toolkit_cmd)
            self._pool.join()
            self._pool.check_results()
        else:
            # Make sure databases exist on destination system
            url = DbURL(self._options.dest_host, self._options.dest_port,
                        'template1', self._options.dest_user)
            conn = connect(url)
            dest_databases = set([db[0] for db in getUserDatabaseList(conn)])
            conn.close()

            needed_databases = \
                set([p.dest.database for p in self._table_transfer_set])

            for database in needed_databases:
                if database not in dest_databases:
                    logger.info('Creating database %s...', database)
                    createdb_cmd = CreateDB('Create database %s' % database,
                                            self._options.dest_host,
                                            self._options.dest_port,
                                            self._options.dest_user,
                                            database)
                    self._pool.addCommand(createdb_cmd)
            self._pool.join()
            self._pool.check_results()

            # Make sure schemas exist
            needed_schemas = dict()
            for p in self._table_transfer_set:
                if p.dest.database not in needed_schemas:
                    needed_schemas[p.dest.database] = list()
                if p.dest.schema not in needed_schemas[p.dest.database]:
                    needed_schemas[p.dest.database].append(p.dest.schema)

            for (database, schemas) in needed_schemas.iteritems():
                url = DbURL(self._options.dest_host, self._options.dest_port,
                            database, self._options.dest_user)
                conn = connect(url)

                for schema in schemas:
                    logger.info('Creating schema %s in database %s...',
                                schema, database)
                    if not doesSchemaExist(conn, schema):
                        execSQL(conn, 'CREATE SCHEMA \"%s\"' % schema)
                conn.commit()
                conn.close()

        # setup the validator
        if self._options.validator:
            logger.info('Setting up validator...')
            validator = validator_factory.get_validator(
                self._options.validator)
            for db in set([p.dest.database for p in self._table_transfer_set]):
                url = DbURL(self._options.dest_host, self._options.dest_port,
                            db, self._options.dest_user)
                conn = connect(url)
                validator.setup(conn)
                conn.commit()
                conn.close()

            for db in set([p.source.database for p in self._table_transfer_set]):
                url = DbURL(
                    self._options.source_host, self._options.source_port,
                    db, self._options.source_user)
                conn = connect(url)
                validator.setup(conn)
                conn.commit()
                conn.close()

        # setup gptransfer schema
        for db in set([p.dest.database for p in self._table_transfer_set]):
            url = DbURL(self._options.dest_host, self._options.dest_port,
                        db, self._options.dest_user)
            conn = connect(url)
            cur = execSQL(conn, 'CREATE SCHEMA gptransfer')
            cur.close()
            conn.commit()
            conn.close()

        for db in set([p.source.database for p in self._table_transfer_set]):
            url = DbURL(self._options.source_host, self._options.source_port,
                        db, self._options.source_user)
            conn = connect(url)
            cur = execSQL(conn, 'CREATE SCHEMA gptransfer')
            cur.close()
            conn.commit()
            conn.close()

        self._cleanup_schemas = True

    def cleanup(self):
        """
        Cleans up the data transfer.
        """
        if not self._options.dry_run:
            # Remove base pipe directory on each source address
            logger.info('Removing work directories...')
            for host in self._host_map.keys():
                address = iter(self._host_map[host]).next()
                cmd = RemoveDirectory('Remove work directory on %s' % address,
                                      self._work_dir, REMOTE, address)
                self._pool.addCommand(cmd)

            cmd = RemoveDirectory('Remove work directory',
                                  self._work_dir, REMOTE, self._options.dest_host)
            self._pool.addCommand(cmd)

            cmd = RemoveDirectory('Remove work directory',
                                  self._work_dir, REMOTE, self._options.source_host)
            self._pool.addCommand(cmd)

            self._pool.join()


            if self._options.validator:
                logger.info('Cleaning up validator...')
                validator = validator_factory.get_validator(
                    self._options.validator)
                # cleanup the validator
                for db in set([p.dest.database for p in self._table_transfer_set]):
                    url = DbURL(
                        self._options.dest_host, self._options.dest_port,
                        db, self._options.dest_user)
                    conn = connect(url)
                    validator.cleanup(conn)
                    conn.commit()
                    conn.close()

                for db in set([p.source.database for p in self._table_transfer_set]):
                    url = DbURL(
                        self._options.source_host, self._options.source_port,
                        db, self._options.source_user)
                    conn = connect(url)
                    validator.cleanup(conn)
                    conn.commit()
                    conn.close()

            # setup gptransfer schema
            if self._cleanup_schemas:
                for db in set([p.dest.database for p in self._table_transfer_set]):
                    url = DbURL(self._options.dest_host, self._options.dest_port,
                                db, self._options.dest_user)
                    conn = connect(url)
                    cur = execSQL(conn, 'DROP SCHEMA gptransfer CASCADE')
                    cur.close()
                    conn.commit()
                    conn.close()

                for db in set([p.source.database for p in self._table_transfer_set]):
                    url = DbURL(
                        self._options.source_host, self._options.source_port,
                        db, self._options.source_user)
                    conn = connect(url)
                    cur = execSQL(conn, 'DROP SCHEMA gptransfer CASCADE')
                    cur.close()
                    conn.commit()
                    conn.close()

            self._pool.haltWork()
            self._pool.joinWorkers()

            if running_gpfdists:
                logger.warn('Some gpfdist processes failed to stop on the source')
                logger.warn('system in a timely manner.  These processes may need to be')
                logger.warn('manually cleaned up.')

        logger.info("Finished.")

    def _dry_run(self):
        """
        Prints to the screen what gptransfer will do without actually doing it.
        """
        # print batch info
        logger.info('Data transfer will use a batch size of %d',
                    self._options.batch_size)
        # print source info
        logger.info('The source GPDB system is %s:%d',
                    self._options.source_host,
                    self._options.source_port)
        # print dest info
        logger.info('The destination GPDB system is %s:%d',
                    self._options.dest_host,
                    self._options.dest_port)
        # print table transfer info
        action = ''
        if self._options.skip_existing:
            action = 'skipped'
        elif self._options.drop:
            action = 'dropped'
        elif self._options.truncate:
            action = 'truncated'

        if self._options.schema_only:
            if self._options.full:
                logger.info(
                    'The entire schema of the source system will be created')
                logger.info(
                    'on the destination system but no data will be transfered.')
            else:
                logger.info(
                    'The following tables will be created on the destination ')
                logger.info('system but no data will be transfered:')
                for pair in self._table_transfer_set:
                    logger.info('  %s', str(pair.dest))
        else:
            logger.info('The following tables will be transfered:')
            for pair in self._table_transfer_set:
                action_msg = ''
                if pair.dest in self._dest_tables:
                    action_msg = '(Destination table exists and will be %s)' % action
                logger.info('  %s -> %s %s',
                            str(pair.source),
                            str(pair.dest),
                            action_msg)

    def _validate_options(self):
        """
        Validates the options passed in to the application.
        """

        logger.info('Validating options...')
        if self._options.batch_size > MAX_BATCH_SIZE \
            or self._options.batch_size < 1:
            raise ProgramArgumentValidationException('Invalid value for --batch-size.  '
                                   '--batch-size must be greater than 0 and '
                                   'less than %s' % MAX_BATCH_SIZE)

        if self._options.sub_batch_size > MAX_SUB_BATCH_SIZE \
            or self._options.sub_batch_size < 1:
            raise ProgramArgumentValidationException('Invalid value for --sub-batch-size.  '
                                   'Must be greater than 0 and less than %s'
                                   % MAX_SUB_BATCH_SIZE)

        if self._options.full and len(self._options.exclude_tables) != 0:
            raise ProgramArgumentValidationException('Cannot specify -T and --full options '
                                   'together')

        if self._options.full and self._options.exclude_input_file:
            raise ProgramArgumentValidationException('Cannot specify -F and --full options '
                                   'together')

        if (self._options.exclude_tables or self._options.exclude_input_file) and not self._excluding_table:
            logger.warning('Found no tables to exclude from transfer table list')
            sys.exit(0)

        if self._options.full and (self._options.truncate or self._options.drop or
                                   self._options.skip_existing):
            raise ProgramArgumentValidationException('Cannot specify --drop, --truncate or '
                                   '--skip-existing with --full option')

        if self._options.truncate and self._options.skip_existing:
            raise ProgramArgumentValidationException('Cannot specify --truncate and '
                                   '--skip-existing together')

        if self._options.truncate and self._options.drop:
            raise ProgramArgumentValidationException('Cannot specify --truncate and '
                                   '--drop together')

        if self._options.skip_existing and self._options.drop:
            raise ProgramArgumentValidationException('Cannot specify --drop and '
                                   '--skip-existing together')

        if self._options.schema_only and self._options.truncate:
            raise ProgramArgumentValidationException('Cannot specify --schema-only and '
                                   '--truncate together')

        if not self._options.source_map_file and \
                self._options.source_host != self._options.dest_host and \
                self._options.source_port != self._options.dest_port:
            raise ProgramArgumentValidationException(
                '--source-addresses-file option is required')

        if self._options.source_map_file and not os.path.isfile(self._options.source_map_file):
            raise ProgramArgumentValidationException('File %s does not exist'
                                   % self._options.source_map_file)

        if not self._options.source_host:
            raise ProgramArgumentValidationException('--source-host option is required.')

        if not self._options.full and len(self._options.tables) == 0 \
            and len(self._options.databases) == 0 \
            and not self._options.input_file:
            raise ProgramArgumentValidationException('One of --full, -f, -t, or -d must be specified')

        if sum(bool(option) for option in (self._options.full,
                                           self._options.databases,
                                           self._options.tables,
                                           self._options.input_file)) != 1:
            raise ProgramArgumentValidationException('Only one of --full, -f, -t, or -d can be specified')

        if self._options.full and self._dest_databases != None and len(self._dest_databases) != 0:
            raise ProgramArgumentValidationException('--full option specified but databases exist '
                                   'in destination system')

        if self._options.base_port <= 1024 or self._options.base_port > 65535:
            raise ProgramArgumentValidationException('Invalid --base-port option')

        if self._options.last_port == -1:
            self._options.last_port = self._options.base_port + 1000

        if self._options.last_port > 65535:
            logger.warn('Invalid --last-port value.  Setting last port to 65535')
            self._options.last_port = 65535

        if self._options.base_port >= self._options.last_port:
            self._options.last_port = self._options.base_port + 1000
            if self._options.last_port > 65535:
                raise ProgramArgumentValidationException('--base-port value causes last port to be invalid.')
            logger.warn('--last-port value is less than --base-port.  Setting last port to %d' % (self._options.last_port))

        if self._options.max_line_length < MIN_GPFDIST_MAX_LINE_LENGTH or \
            self._options.max_line_length > MAX_GPFDIST_MAX_LINE_LENGTH:
            raise ProgramArgumentValidationException('Invalid --max-line-length option.  Value must be '
                                   'between %d and %d' % (MIN_GPFDIST_MAX_LINE_LENGTH,
                                                          MAX_GPFDIST_MAX_LINE_LENGTH))

        if self._options.timeout < MIN_GPFDIST_TIMEOUT or \
            self._options.timeout > MAX_GPFDIST_TIMEOUT:
            raise ProgramArgumentValidationException('Invalid --timeout option.  Value must be between '
                                   '%d and %d' % (MIN_GPFDIST_TIMEOUT, MAX_GPFDIST_TIMEOUT))

        if self._options.wait_time < 0:
            raise ProgramArgumentValidationException('--wait-time must be greater than 0')

        self._gphome = get_gphome()
        if not FileDirExists.remote('remote gphome check',
                                    self._options.source_host, self._gphome):
            raise ProgramArgumentValidationException('GPHOME directory does not exist on %s'
                                   % self._options.source_host)

        if self._gpfdist_instance_count == 0:
            raise Exception('Destination system is too small')

        if self._options.validator and self._options.validator not in \
                validator_factory.get_available_validators():
            raise ProgramArgumentValidationException(
                'Unknown validator %s' % self._options.validator)

        if self._options.source_host == self._options.dest_host and \
                self._options.source_port == self._options.dest_port and \
                not self._options.dest_database:
            raise ProgramArgumentValidationException('Source and destination systems cannot be '
                                   'the same unless --dest-database option is '
                                   'set')

        if self._options.full and self._options.dest_database:
            raise ProgramArgumentValidationException('--dest-database option cannot be used with'
                                   ' the --full option')

        if (self._options.delimiter[0] == "\\" and len(self._options.delimiter) != 4) and \
           (len(self._options.delimiter) != 1):
            raise ProgramArgumentValidationException(
                'Invalid --delimiter.  Must be single character')

        if self._options.delimiter != "," and self._options.format.lower() == 'csv':
            raise ProgramArgumentValidationException(
                'Invalid --delimiter. Delimiter %s is not allowed in CSV(default) format. Specify --format=TEXT' % self._options.delimiter)

        if self._options.delimiter == ',' and self._options.format.lower() == 'text':
            raise ProgramArgumentValidationException(
                'Invalid --delimiter. Default delimiter "," is not allowed in TEXT format. Specify --delimiter option for TEXT')

    def _get_configurations(self):
        """
        Retrieves the configurations of the source and destination GPDB systems.
        """

        logger.info('Retrieving configuration of source Greenplum Database...')
        url = DbURL(self._options.source_host,
                    self._options.source_port, 'template1',
                    self._options.source_user)
        self._src_config = GpArray.initFromCatalog(url)
        logger.info('Retrieving configuration of destination Greenplum '
                    'Database...')
        url = DbURL(self._options.dest_host, self._options.dest_port,
                    'template1', self._options.dest_user)
        self._dest_config = GpArray.initFromCatalog(url)


    def _validate_schema_not_exists(self):
        """
        Check to see if we have any gptransfer schemas on either system
        """
        logger.info('Checking for gptransfer schemas...')

        schemas_exists_on_source = False
        schemas_exists_on_destination = False
        if self._options.full:
            if schema_exists_on_system(self._options.source_host,
                                       self._options.source_port,
                                       self._options.source_user,
                                       'gptransfer'):
                schemas_exists_on_source = True

            if schema_exists_on_system(self._options.dest_host,
                                       self._options.dest_port,
                                       self._options.dest_user,
                                       'gptransfer'):
                schemas_exists_on_destination = True

        else:
            if schema_exists_on_system(self._options.source_host,
                                       self._options.source_port,
                                       self._options.source_user,
                                       'gptransfer',
                                       self._src_databases):
                schemas_exists_on_source = True

            if schema_exists_on_system(self._options.dest_host,
                                       self._options.dest_port,
                                       self._options.dest_user,
                                       'gptransfer',
                                       [db for db in self._dest_databases if db in self._all_dest_databases]):
                schemas_exists_on_destination = True

        if schemas_exists_on_source:
            logger.warning('The gptransfer schema already exists on the source system.')
            logger.warning('This is likely due to a previous run on gptransfer')
            logger.warning('being forcefully terminated and not properly cleaned up.')
            logger.warning('Removing existing gptransfer schema on source system.')
            if self._options.full:
                drop_existing_schema_on_system(self._options.source_host,
                                               self._options.source_port,
                                               self._options.source_user,
                                               'gptransfer')
            else:
                drop_existing_schema_on_system(self._options.source_host,
                                               self._options.source_port,
                                               self._options.source_user,
                                               'gptransfer',
                                               self._src_databases)

        if schemas_exists_on_destination:
            logger.warning('The gptransfer schema already exists on destination system.')
            logger.warning('This is likely due to a previous run on gptransfer')
            logger.warning('being forcefully terminated and not properly cleaned up.')
            logger.warning('Removing existing gptransfer schema on destination system.')
            if self._options.full:
                drop_existing_schema_on_system(self._options.dest_host,
                                               self._options.dest_port,
                                               self._options.dest_user,
                                               'gptransfer')
            else:
                drop_existing_schema_on_system(self._options.dest_host,
                                               self._options.dest_port,
                                               self._options.dest_user,
                                               'gptransfer',
                                               [db for db in self._dest_databases if db in self._all_dest_databases])


    def _get_source_tables(self):
        source_tables = list()
        if self._options.full:
            for src_table in get_user_tables(self._options.source_host,
                                             self._options.source_port,
                                             self._options.source_user,
                                             full = True):
                if src_table.database != 'gpperfmon':
                    source_tables.append(src_table)

            self._src_databases = self._all_src_databases
            self._dest_databases = self._all_dest_databases

            if 'gpperfmon' in self._src_databases:
                logger.warn('Tables in database "gpperfmon" will not be transferred with full option')
                self._src_databases.remove('gpperfmon')
        else:
            # get all source databases for both full transfer and table transfer
            databases_full_transfer = list()
            databases_table_transfer = list()

            # for databases to transfer with option -d
            databases = list()
            for database in self._options.databases:
                databases.append(_process_regexp(database))
            databases_full_transfer = get_databases_by_regexp(
                                               self._options.source_host,
                                               self._options.source_port,
                                               self._options.source_user,
                                               databases, True)

            # add all tables of source databases
            for table in get_user_tables(self._options.source_host,
                                         self._options.source_port,
                                         self._options.source_user,
                                         databases_full_transfer):
                source_tables.append(table)

            del databases[:]
            # add specified tables and parse wildcard of table names
            tables = list()
            for fqn in self._options.tables:
                (database, schema, table) = split_fqn(fqn)
                database = _process_regexp(database)
                schema = _process_regexp(schema)
                table = _process_regexp(table)

                if database not in databases:
                    databases.append(database)

                if (database, schema, table) not in tables:
                    tables.append((database, schema, table))

            # add all tables in input file and parse wildcard of table names
            # gptransfer with -f option
            if self._options.input_file:
                with open(self._options.input_file) as input_file:
                    for line in input_file:
                        line = line.strip()
                        if line.startswith('#'):
                            continue

                        (database, schema, table) = split_fqn(line)
                        database = _process_regexp(database)
                        schema = _process_regexp(schema)
                        table = _process_regexp(table)

                        if database not in databases:
                            databases.append(database)

                        if (database, schema, table) not in tables:
                            tables.append((database, schema, table))

            databases_table_transfer = get_databases_by_regexp(
                                           self._options.source_host,
                                           self._options.source_port,
                                           self._options.source_user,
                                           databases, True)

            # get all source databases to transfer tables from
            self._src_databases = databases_full_transfer + databases_table_transfer

            # get all destination databases
            if self._options.dest_database:
                self._dest_databases = [self._options.dest_database]
            else:
                self._dest_databases = [db for db in self._all_dest_databases if db in self._src_databases]

            # get all the tables of specified databases from the source GPDB system
            logger.info("Retrieving source tables...")
            user_tables = get_user_tables(self._options.source_host,
                                          self._options.source_port,
                                          self._options.source_user,
                                          databases_table_transfer)

            # Filter down to only the tables needed to added
            seen = set()
            for (db_, schema_, table_) in tables:
                for user_table in user_tables:
                    database, schema, table = user_table.database, user_table.schema, user_table.table

                    try:
                        if (re.match(r'%s' % db_, database, re.L|re.U) and
                            re.match(r'%s' % schema_, schema, re.L|re.U) and
                            re.match(r'%s' % table_, table, re.L|re.U)):
                                if user_table not in seen:
                                    seen.add(user_table)
                                    source_tables.append(user_table)

                    except Exception, e:
                        raise Exception('Error matching regular expression, %s with database name %s'
                                        ' or %s with schema name %s, or %s with table name %s\n%s' %
                                        (db_, database, schema_, schema, table_, table, e))

        return self.filter_tables(source_tables, self.get_exclude_tables())


    def get_exclude_tables(self):
        """
        get all tables to exclude from transfer table list
        """

        exclude_tables = set()

        # excluded tables
        for fqn in self._options.exclude_tables:
            (database, schema, table) = split_fqn(fqn)
            database = _process_regexp(database)
            schema = _process_regexp(schema)
            table = _process_regexp(table)
            exclude_tables.add((database, schema, table))

        # add all tables in exclude input file and parse wildcard of table names
        if self._options.exclude_input_file:
            with open(self._options.exclude_input_file) as input_file:
                for line in input_file:
                    line = line.strip()
                    if line.startswith('#'):
                        continue

                    (database, schema, table) = split_fqn(line)
                    database = _process_regexp(database)
                    schema = _process_regexp(schema)
                    table = _process_regexp(table)

                    exclude_tables.add((database, schema, table))

        return exclude_tables 


    def filter_tables(self, source_tables, exclude_tables):
        """
        filter out the source tables from transferring
        """
        if (source_tables is None or len(source_tables) == 0 or
            exclude_tables is None or len(exclude_tables) == 0):

            return source_tables

        tables_to_transfer = list()
        for src_table in  source_tables:
            database, schema, table = src_table.database, src_table.schema, src_table.table

            is_exclude_table_match = False
            for (db_, schema_, table_) in exclude_tables:
                try:
                    if (re.match(r'%s' % db_, database, re.L|re.U) and
                        re.match(r'%s' % schema_, schema, re.L|re.U) and
                        re.match(r'%s' % table_, table, re.L|re.U)):

                        is_exclude_table_match = True
                        break
                except Exception, e:
                    raise Exception('Error matching regular expression, %s with database name %s'
                                    ' or %s with schema name %s, or %s with table name %s\n%s' %
                                    (db_, database, schema_, schema, table_, table, e))

            if is_exclude_table_match:
                logger.debug("Excluding table %s", src_table)
            else:
                logger.debug('Adding table %s', src_table)
                tables_to_transfer.append(src_table)

        if len(tables_to_transfer) == len(source_tables):
            self._excluding_table = False

        return tables_to_transfer

    def _build_table_transfer_set(self):
        """
        Builds up the list of tables to transfer.
        """

        logger.info('Building list of source tables to transfer...')
        table_pairs = list()
        seen = set()

        for src_table in self._src_tables:
            dest_database = src_table.database
            if self._options.dest_database and not self._options.full:
                dest_database = self._options.dest_database

            table_pair = GpTransferTablePair(src_table,
                                             GpTransferTable(dest_database,
                                                             src_table.schema,
                                                             src_table.table,
                                                             src_table.external))
            if table_pair not in seen:
                seen.add(table_pair)
                table_pairs.append(table_pair)

        logger.info('Number of tables to transfer: %s', len(table_pairs))

        return table_pairs


    def _is_identifier_supported(self, identifier):
        """
        Verify that the characters of identifier starts with a-z, or A-Z or underscore,
        followed by lower or upper case (a-z, A-Z), or digits(0-9), or underscore (_)
        """

        if not ((identifier[0:1] >= 'a' and identifier[0:1] <= 'z') or
                (identifier[0:1] >= 'A' and identifier[0:1] <= 'Z') or
                identifier[0:1] == '_'):

           return False

        for c in identifier:
            if not ((c >= 'a' and c <= 'z') or
                    (c >= 'A' and c <= 'Z') or
                    (c >= '0' and c <= '9') or
                    (c == '_')):
                return False
        return True

    def _validate_table_transfer_set(self):
        """
        Validates the table transfers, checking if tables exist, etc.
        """

        logger.info('Validating transfer table set...')

        # make sure we don't collide on dest table names
        non_supported_table_name_list = list()
        target_tables = set()
        remove_list = list()
        truncate_list = list()
        drop_list = list()

        for table_pair in self._table_transfer_set:

            if not (self._is_identifier_supported(table_pair.source.database) and
                    self._is_identifier_supported(table_pair.source.schema) and
                    self._is_identifier_supported(table_pair.source.table)):

                non_supported_table_name_list.append(table_pair)

            if table_pair.dest in target_tables:
                raise Exception('Multiple tables map to %s.  Remove one of '
                                'the tables ' % table_pair.dest
                                + 'from the list or do not use the '
                                '--dest-database option.')
            target_tables.add(table_pair.dest)

            if table_pair.dest in self._dest_tables:
                if self._options.skip_existing:
                    remove_list.append(table_pair)

                    #remove tables with non supported name if not to transfer
                    if table_pair in non_supported_table_name_list:
                        non_supported_table_name_list.remove(table_pair)
                elif table_pair.dest.external:
                    continue
                elif not self._options.truncate and not self._options.drop:
                    raise Exception('Table %s exists in database %s.'
                                    % (table_pair.dest,
                                    table_pair.dest.database))
                else:
                    if self._options.truncate:
                        truncate_list.append(table_pair.dest)
                    else:
                        drop_list.append(table_pair.dest)

        if len(non_supported_table_name_list) > 0:
            with open(GPTRANSFER_FAILED_TABLES_FILE, 'w') as failed_file:
                for table_pair in non_supported_table_name_list:
                    failed_file.write("%s%s%s%s%s\n" %
                                     (table_pair.source.database, SCHEMA_DELIMITER,
                                      table_pair.source.schema, SCHEMA_DELIMITER, table_pair.source.table))
            logger.error('Found unsupported identifiers')
            logger.error('Valid identifiers must start with a-z, or A-Z or underscore followed by a-z, '
                         'or A-Z, or 0-9, or underscore')
            logger.error('A list of these tables has been written to the file %s', GPTRANSFER_FAILED_TABLES_FILE)
            sys.exit(1)

        for remove in remove_list:
            self._table_transfer_set.remove(remove)

        if len(self._table_transfer_set) == 0 and not self._options.dry_run:
            logger.info('Found no tables to transfer.')
            sys.exit(0)

        if self._options.interactive and not self._options.dry_run:
            ask = False
            if self._options.truncate and len(truncate_list) > 0:
                ask = True
                logger.info(
                    'The following tables on the destination system will be truncated:')
                for table in truncate_list:
                    logger.info('   %s', str(table))

            elif self._options.drop and len(drop_list) > 0:
                ask = True
                logger.info(
                    'The following tables on the destination system will be dropped:')
                for table in drop_list:
                    logger.info('   %s', str(table))

            if ask:
                cont = ask_yesno(None, 'Do you want to continue?', 'N')
                if not cont:
                    raise Exception('Canceled')

    def _get_host_map(self):
        """
        Reads in the host map file.

        The format of the input file is:
            hostname1,addr1[,addr2...]
            ...
            hostnameN,addr1[,addr2...]

        Can use a # to comment a line.
        """
        host_map = None

        if self._options.source_map_file:
            logger.info('Reading source host map file...')
            with open(self._options.source_map_file) as map_file:
                host_map = dict()
                for line in map_file:
                    line = line.strip()
                    if line.startswith('#') or len(line) == 0:
                        continue
                    line_list = line.split(',')
                    (hostname, addresses) = (line_list[0], line_list[1:])
                    if hostname not in host_map:
                        host_map[hostname] = set()
                    map(host_map[hostname].add, [address.strip()
                        for address in addresses])
        elif self._options.source_host == self._options.dest_host and \
                self._options.source_port == self._options.dest_port and \
                self._options.dest_database and not self._options.full:
            # src and dest are same so we don't need map file, we can build it
            host_map = dict()
            for seg in self._src_config.getSegDbList():
                hostname = seg.getSegmentHostName()
                if seg.isSegmentMaster() or hostname in host_map:
                    continue
                host_map[hostname] = [hostname]
        else:
            raise Exception('--source-map-file option must be used when '
                            'source and destination systems are not the same')

        return host_map

    def _validate_host_map(self):
        """
        Validates that all hosts in the source system have an entry in the map
        file.  It validates against the hostname column in
        gp_segment_configuration catalog table.
        """
        logger.info('Validating source host map...')

        if not self._host_map or len(self._host_map) == 0:
            raise Exception('No hosts in map')

        for seg in self._src_config.getSegDbList():
            if seg.isSegmentMaster():
                continue
            seg_host = seg.getSegmentHostName()
            if seg_host not in self._host_map:
                raise Exception('Hostname %s missing from map file' % seg_host)
            if len(self._host_map[seg_host]) == 0:
                raise Exception('No addresses for hostname %s' % seg_host)


# --------------------------------------------------------------------------
# Main
# --------------------------------------------------------------------------
if __name__ == '__main__':
    if curr_platform != LINUX:
        logger.error("gptransfer is only available on Linux.")
        sys.exit(1)

    # Preemptively check for command -h | --help.
    # This will automacially exit gptransfer and print the help text if -h | --help is found
    exit_on_help = create_parser()
    exit_on_help.parse_args()

    sys.argv[0] = EXECNAME
    simple_main(create_parser, GpTransfer,
                {'pidfilename': GPTRANSFER_PID_FILE,
                'programNameOverride': EXECNAME})
